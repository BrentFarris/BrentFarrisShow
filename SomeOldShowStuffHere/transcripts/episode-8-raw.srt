1
00:00:00,000 --> 00:00:04,440
Welcome back! Not a whole lot of tech news over the weekend or Monday. I meant

2
00:00:04,440 --> 00:00:08,560
to put out a video on Monday but I was sidetracked training AI and learning Go.

3
00:00:08,560 --> 00:00:17,600
But 1.3 billion dollars is how much Facebook is being fined. Which I think

4
00:00:17,600 --> 00:00:25,480
they made 23 billion dollars like last year or maybe this year so far. I'm not

5
00:00:25,480 --> 00:00:32,960
sure. Oh 28 billion. 28 billion in the first three months of this year. So yes

6
00:00:32,960 --> 00:00:38,000
they've made 28 billion and they're being fined 1.3 billion. Of course 1.3

7
00:00:38,000 --> 00:00:43,000
billion is probably the GDP of some small country. So it's a lot of money for

8
00:00:43,000 --> 00:00:49,240
somebody but not a lot of money for Facebook. Now why are they being fined

9
00:00:49,240 --> 00:00:55,920
over a billion dollars? Well they're being fined because they were transmitting

10
00:00:55,920 --> 00:01:06,520
data from the EU into the United States. Arguably the one of the at least top

11
00:01:06,520 --> 00:01:11,360
five freest countries in the world. I know I know. But at the same time they

12
00:01:11,360 --> 00:01:15,240
are big corporations and I am a huge advocate of not giving your your

13
00:01:15,240 --> 00:01:22,000
information to big corporations and also not giving them basically everything

14
00:01:22,000 --> 00:01:26,880
about your life. Now whether you choose to do that willingly is a different

15
00:01:26,880 --> 00:01:31,080
question. A lot of people do use these services and upload their photos and

16
00:01:31,080 --> 00:01:36,520
include geolocation and say that I'm okay whenever some natural disaster

17
00:01:36,520 --> 00:01:41,200
happens and says and they do check-ins where they check into some place they've

18
00:01:41,200 --> 00:01:46,760
been. So they they have to know. People do have to know that that data is going out

19
00:01:46,760 --> 00:01:55,240
to somebody like their friends. They're posting it to somewhere. Now I should I

20
00:01:55,240 --> 00:02:00,600
should think that everything should have a little bit of security to it but our

21
00:02:00,600 --> 00:02:08,160
privacy is a commodity to big companies and advertising agencies and AI training

22
00:02:08,160 --> 00:02:12,720
people. People who train AI are definitely gonna be the new the new

23
00:02:12,720 --> 00:02:16,760
people who want your data by the way and Facebook is one of those people

24
00:02:16,760 --> 00:02:24,600
training AI. In fact PyTorch one of the top two AI training frameworks is

25
00:02:24,600 --> 00:02:30,280
developed by Facebook. In fact it's it's competing for number one spot with like

26
00:02:30,280 --> 00:02:37,360
TensorFlow and they're constantly in flux with each other. So yeah data is is

27
00:02:37,360 --> 00:02:45,880
a big thing and so the EU is is charging Facebook this 1.3 billion dollars

28
00:02:45,880 --> 00:02:51,400
because it accuses it of violating privacy rules and snooping. Now of course

29
00:02:51,400 --> 00:02:54,320
these companies have developed their architecture to not really care about

30
00:02:54,320 --> 00:02:58,000
what country you're in and they don't block off stuff you know unless it's in

31
00:02:58,000 --> 00:03:03,040
China. They do really good job of filtering stuff real quick there.

32
00:03:03,040 --> 00:03:09,520
Ultimately I'm not you know I'm not super up in arms about this. I do think

33
00:03:09,520 --> 00:03:17,960
that company that that countries finding companies is a way of tax or something

34
00:03:17,960 --> 00:03:21,040
like that. I don't I don't see what they're trying to get at here because

35
00:03:21,040 --> 00:03:25,680
let me let me just give you a little antidote about these big corporations. I

36
00:03:25,680 --> 00:03:33,720
worked at a pretty big game company in the past and there was a problem where

37
00:03:33,720 --> 00:03:39,440
people were using old iPads and there was a pretty significant amount of

38
00:03:39,440 --> 00:03:45,440
people using these old iPads for the application that they were working on

39
00:03:45,440 --> 00:03:51,960
and or that the application the game that we produced there there's tons of

40
00:03:51,960 --> 00:03:55,200
people on this and so they were valuable we didn't want to just cut them off

41
00:03:55,200 --> 00:04:00,480
of being able to play. We wanted those people to continue playing because it

42
00:04:00,480 --> 00:04:06,640
they generated revenue. Now the cost to developers to continue to try and

43
00:04:06,640 --> 00:04:12,920
support these old iPads was pretty expensive. So one of the talks that the

44
00:04:12,920 --> 00:04:19,400
company went about in was just sending every single customer on that version

45
00:04:19,400 --> 00:04:23,880
iPad a new iPad. They were just gonna buy a new iPad for all of the customers that

46
00:04:23,880 --> 00:04:30,440
played the game using that old model and the reason this was a discussion is

47
00:04:30,440 --> 00:04:35,720
because the one of the companies that was also equally large some employees

48
00:04:35,720 --> 00:04:40,440
came from did this exact thing and it saved them on development costs and all

49
00:04:40,440 --> 00:04:44,920
this sort of stuff. Why is this important to the Facebook thing? 1.3 billion

50
00:04:44,920 --> 00:04:50,320
dollars for being able to have access to all the information about people in the

51
00:04:50,320 --> 00:04:58,280
EU? It's just what they would consider a cost of doing business. So I can see that

52
00:04:58,280 --> 00:05:05,880
that the value to them to be able to keep this this data transfer going and

53
00:05:05,880 --> 00:05:13,280
not segment their platform is it's worth the cost. So they're not this this 1.3

54
00:05:13,280 --> 00:05:16,360
billion dollars isn't going to make them ultimately change how they do

55
00:05:16,360 --> 00:05:20,080
everything. Of course legislation will if they try to isolate all the

56
00:05:20,080 --> 00:05:25,960
servers in Europe and I would argue that's a good way to close

57
00:05:25,960 --> 00:05:31,240
off the internet and yes I know we're dealing with corporations but what if I

58
00:05:31,240 --> 00:05:38,280
a humble American in freedom land decided that I wanted to set up a

59
00:05:38,280 --> 00:05:43,200
company that was worldwide some language learning thing or I don't know some

60
00:05:43,200 --> 00:05:47,360
something to do your taxes. I guess tax is a bad thing because it's kind of

61
00:05:47,360 --> 00:05:55,920
regional. Something I wanted to sell a game and my game had game servers. Now I

62
00:05:55,920 --> 00:06:00,000
do want to set up a server over in the EU so that it's faster but I want to

63
00:06:00,000 --> 00:06:03,920
store all the data collectively in the United States with all my other data. I

64
00:06:03,920 --> 00:06:08,400
want to aggregate it into a central location so it's faster for processing

65
00:06:08,400 --> 00:06:13,680
and searching. So if somebody wants to search for some result like they want to

66
00:06:13,680 --> 00:06:16,920
look for a player on the other side of the planet they don't have to do a

67
00:06:16,920 --> 00:06:20,160
round-trip way over there it's all in one place and it's locally cached.

68
00:06:20,160 --> 00:06:26,920
Anyways semantics aside it would be very difficult for me to kind of avoid that

69
00:06:26,920 --> 00:06:32,040
law. It kind of makes the big bigger. It makes all the little people who want to

70
00:06:32,040 --> 00:06:37,320
start up a business be cut out of the business because now they can't serve

71
00:06:37,320 --> 00:06:43,840
anyone in Europe. It seems like you know big companies like you know Facebook and

72
00:06:43,840 --> 00:06:49,320
Google and Microsoft and all of them will easily be able to to manage all of

73
00:06:49,320 --> 00:06:53,520
that bureaucracy but the little guys won't. And that's usually how this

74
00:06:53,520 --> 00:06:57,880
legislation for these kinds of things go is it destroys the opportunity for small

75
00:06:57,880 --> 00:07:03,680
companies and it doesn't hurt these big guys. They just get huge and they just it

76
00:07:03,680 --> 00:07:06,480
just cuts out the competition for them to have to fight these battles. And of

77
00:07:06,480 --> 00:07:11,360
course they they just litigate you know and they do appeals and it takes forever

78
00:07:11,360 --> 00:07:16,080
for them to pay and they wind up paying less and all this other stuff. So I don't

79
00:07:16,080 --> 00:07:19,800
really see the value in this kind of thing. So that was one of the things that

80
00:07:19,800 --> 00:07:27,840
came up. And speaking of overseas servers and overseas data I also read that

81
00:07:27,840 --> 00:07:35,040
there is a leaked document from from from Europe again. I love I love my

82
00:07:35,040 --> 00:07:40,360
cousins in the Europe but you know some of the legislation is it's kind of crazy.

83
00:07:40,360 --> 00:07:48,400
There was a document about how a lot of lawmakers want to ban end-to-end

84
00:07:48,400 --> 00:07:53,360
encryption. They want to keep it but they want to ban it. They want to be able to

85
00:07:53,360 --> 00:07:58,520
read everybody's stuff to make sure they're not doing anything bad but they

86
00:07:58,520 --> 00:08:03,360
also want it to stay secure. It feels like these people have no idea what

87
00:08:03,360 --> 00:08:08,160
technology is. They've used a phone and they think they're experts or something.

88
00:08:08,160 --> 00:08:13,480
The article was it was specifically pointing at Spain because Spain really

89
00:08:13,480 --> 00:08:17,920
wanted to just ban end-to-end encryption totally and this was a leaked document

90
00:08:17,920 --> 00:08:23,560
or something like that. But there is it talked about how many members of the EU

91
00:08:23,560 --> 00:08:29,760
were doing proposals. So it says it reveals a strong strong support among

92
00:08:29,760 --> 00:08:35,840
EU member states for proposals to scan private messages for illegal content.

93
00:08:35,840 --> 00:08:42,880
Another part once this right here this is it's like a wolf in sheep's

94
00:08:42,880 --> 00:08:48,280
clothing. Craft a highly controversial law to stop the spread of child sexual

95
00:08:48,280 --> 00:08:54,240
abuse material in Europe. Now if Europe has a problem with child sexual abuse as

96
00:08:54,240 --> 00:08:59,600
every country in the world does they should definitely go to the fullest

97
00:08:59,600 --> 00:09:03,600
extensive law put those people in jail for life. I don't care what happens to

98
00:09:03,600 --> 00:09:11,280
those people. People who abuse children can rot for all I care. But this is often

99
00:09:11,280 --> 00:09:16,480
a tactic that they used. You'll remember I iPhone did this. They they started

100
00:09:16,480 --> 00:09:20,320
scanning people's libraries. If you had an iPhone I don't know if

101
00:09:20,320 --> 00:09:23,840
they still do it. I think they probably still do it. They scan all your photos

102
00:09:23,840 --> 00:09:27,320
and then report you to somebody if you have any kind of suspect photos. And

103
00:09:27,320 --> 00:09:31,720
their machine learning algorithms are horrid. There's all kinds of

104
00:09:31,720 --> 00:09:35,920
articles that came out when they launched it about people who would take

105
00:09:35,920 --> 00:09:40,600
pictures of their kids who were I don't know they fell off a slide or something

106
00:09:40,600 --> 00:09:44,760
and they were taking a picture to send to their doctor and they were flagged

107
00:09:44,760 --> 00:09:48,480
and the police showed up there at their door saying that they were child abusers.

108
00:09:48,480 --> 00:09:56,320
And so first of all it doesn't even work and second of all I don't think this is

109
00:09:56,320 --> 00:10:00,520
the most effective way to catch them. I think that it's a an excuse. It's kind of

110
00:10:00,520 --> 00:10:04,120
like how politicians just make up crap. They make up random titles that have

111
00:10:04,120 --> 00:10:07,880
nothing to do with bills in order to make everyone kind of get on the

112
00:10:07,880 --> 00:10:11,840
bandwagon with the name of the bill. It's the same kind of thing. Oh we want to

113
00:10:11,840 --> 00:10:15,200
create technology to scan to scan people's phones just to make sure there

114
00:10:15,200 --> 00:10:23,120
isn't or any kind of child abuse. And that's not exactly real. I don't

115
00:10:23,120 --> 00:10:27,200
think that's what it's for. I think it's a backdoor into people's private lives

116
00:10:27,200 --> 00:10:30,120
so that you can get more advertising money so that artificial intelligence

117
00:10:30,120 --> 00:10:35,920
can be better trained and so on and so forth. And you're using these presumably

118
00:10:35,920 --> 00:10:43,080
what would be high morals, these morals that anyone could agree to, but you're

119
00:10:43,080 --> 00:10:50,840
kind of using it as a facade to what you're actually doing. So there's another

120
00:10:50,840 --> 00:10:54,920
section in here that I highlighted. It said for years EU states have debated

121
00:10:54,920 --> 00:10:58,480
whether end-to-end encryption communication platforms such as WhatsApp

122
00:10:58,480 --> 00:11:03,840
and Signal should be protected as a way for Europeans to

123
00:11:03,840 --> 00:11:09,760
exercise a fundamental right to privacy or weakened to keep criminals from being

124
00:11:09,760 --> 00:11:16,080
able to communicate outside of the reach of the law enforcement. Now I get it.

125
00:11:16,080 --> 00:11:18,480
There's bad people everywhere and they're gonna find a way to talk. They'll

126
00:11:18,480 --> 00:11:21,800
use carrier pigeons and what are you gonna do? Scan the birds in the air? I

127
00:11:21,800 --> 00:11:26,960
mean with drones you probably already are. But I don't know. I'm a big person

128
00:11:26,960 --> 00:11:31,400
for privacy and security. For example when it comes to AI the reason I like

129
00:11:31,400 --> 00:11:36,400
I'm interested in AI and I work with AI and I train AI models now is because I'm

130
00:11:36,400 --> 00:11:42,620
interested in a private personal artificial intelligence that is not run

131
00:11:42,620 --> 00:11:46,640
by a company. Any company. It doesn't matter what if their name is OpenAI or

132
00:11:46,640 --> 00:11:51,480
not. Open sounds open but you know when you have tens of billions of dollars

133
00:11:51,480 --> 00:11:58,080
coming in from Microsoft you kind of you are gonna lean in one direction. So you

134
00:11:58,080 --> 00:12:03,960
know share a little too much information. So I am for all for personal encryption

135
00:12:03,960 --> 00:12:07,560
and yes there is a cost to it. There's a cost to freedom. There's always a cost to

136
00:12:07,560 --> 00:12:11,800
freedom. We all know it. We all can read our history books. There's always a cost

137
00:12:11,800 --> 00:12:18,880
to freedom and you know you can't you can't have both. Let's talk a little bit

138
00:12:18,880 --> 00:12:23,280
about government surveillance. This is something that has been coming up

139
00:12:23,280 --> 00:12:29,400
recently. It's a thing about governments wanting to use

140
00:12:29,400 --> 00:12:36,400
artificial intelligence to not exactly track people but to track people and you

141
00:12:36,400 --> 00:12:39,720
know predict things like when car crashes are going to happen or when

142
00:12:39,720 --> 00:12:44,760
somebody's doing something bad or whatever. Of course this is dystopian and

143
00:12:44,760 --> 00:12:51,000
everyone knows the references to what this is like but it's not just communist

144
00:12:51,000 --> 00:12:55,480
countries like China that are that are trying to impose these. It is freedom

145
00:12:55,480 --> 00:13:01,920
countries like Europe and even parts of the United States that want to monitor

146
00:13:01,920 --> 00:13:10,920
people. Now of course every government in world history is afraid of its

147
00:13:10,920 --> 00:13:15,280
own people in some way or another. The biggest danger is the danger

148
00:13:15,280 --> 00:13:20,960
within and you can see this in a lot of wars nobody at least recently nobody has

149
00:13:20,960 --> 00:13:24,720
conquered another country. What they do is they go fight the war and then when

150
00:13:24,720 --> 00:13:29,320
they pull out the people inside bubble up and change how things operate. Now

151
00:13:29,320 --> 00:13:33,880
that's kind of the MO of stuff. That's the MO of pretty much any nation even

152
00:13:33,880 --> 00:13:41,440
throughout history. So it does make sense that government bodies people who want

153
00:13:41,440 --> 00:13:47,240
to stay employed and one of the ways that people stay employed in the

154
00:13:47,240 --> 00:13:51,240
government is by monitoring the people and knowing what's going on at all times

155
00:13:51,240 --> 00:13:56,440
so that they can better prepare for disasters and you know that sort of

156
00:13:56,440 --> 00:14:02,720
stuff. So I never buy it. I am I'm kind of in the camp of leave me alone I'll

157
00:14:02,720 --> 00:14:11,360
leave you alone and we'll all be happy. Let us just let us all just talk on the

158
00:14:11,360 --> 00:14:15,440
internet play some video games hang out. There's criminals there's always

159
00:14:15,440 --> 00:14:18,560
criminals but you know what there's more good people than there are criminals.

160
00:14:18,560 --> 00:14:27,480
It's just just look at the ratio of people even in you know places that

161
00:14:27,480 --> 00:14:30,720
have governments that aren't the greatest. There's still a huge ratio of

162
00:14:30,720 --> 00:14:38,760
good people. So that's kind of the perspective I have on governments

163
00:14:38,760 --> 00:14:44,880
trying to invest in AI but but there are there are serious consequences to to

164
00:14:44,880 --> 00:14:51,560
this concept of AI trained government systems whether it's in law enforcement

165
00:14:51,560 --> 00:14:59,040
whether it's in you know tracking down people for tax evasion or whether it's

166
00:14:59,040 --> 00:15:03,320
for I don't know just I don't want to walk into a building worrying about my

167
00:15:03,320 --> 00:15:07,680
face being scanned and sent off to to somebody and then me accidentally. I mean

168
00:15:07,680 --> 00:15:12,880
how many how many people have been accidentally jailed even for a short

169
00:15:12,880 --> 00:15:17,760
period of time because messed up you know somebody mistake the identity.

170
00:15:17,760 --> 00:15:21,920
Imagine how much more that would be you know AI is like oh well it's it looks

171
00:15:21,920 --> 00:15:24,800
like the guy because there's I mean there's a lot of people that look like

172
00:15:24,800 --> 00:15:27,520
me and there's a lot of people that look like you. That's just how it is you know

173
00:15:27,520 --> 00:15:33,840
we're all related in some way right. Like I have two parents my wife has two

174
00:15:33,840 --> 00:15:38,880
parents that's already four so you can see where this is going and if you take

175
00:15:38,880 --> 00:15:42,640
if you have two parents for each one of those people right you now have eight

176
00:15:42,640 --> 00:15:48,600
and it just it scales up really fast in just a few generations you would have

177
00:15:48,600 --> 00:15:55,400
more people than all of human history has ever existed in just a few

178
00:15:55,400 --> 00:16:00,520
generations. So yeah a lot of people look like each other it's just how it works.

179
00:16:00,520 --> 00:16:06,480
You can you can maybe tell the difference or in a small group like in

180
00:16:06,480 --> 00:16:12,200
a in a town or something you can really focus on someone and say ah this guy is

181
00:16:12,200 --> 00:16:16,560
highly likely the guy because it looks like him on the camera. You deploy that

182
00:16:16,560 --> 00:16:22,200
like to an FBI database and I walk into a government facility where I live but

183
00:16:22,200 --> 00:16:27,400
the crime was you know ten states over or something and it dings the FBI says

184
00:16:27,400 --> 00:16:31,920
here's your guy because there's 300 million people here there's gonna be

185
00:16:31,920 --> 00:16:38,640
some mess-ups. So I really don't want to see the government getting more into AI

186
00:16:38,640 --> 00:16:43,480
for policing or anything any other reason. I don't think there's any

187
00:16:43,480 --> 00:16:49,080
business for government to be an AI at all. I mean they can that that's why we

188
00:16:49,080 --> 00:16:53,880
have the private sector right so they can pay somebody if your worry is that

189
00:16:53,880 --> 00:16:58,280
hey people are gonna be using AI to do bad things and you need to use AI to

190
00:16:58,280 --> 00:17:05,200
combat it then you have the private sector. We pay taxes you waste our money

191
00:17:05,200 --> 00:17:08,800
on the private sector and they create AI to combat it that's how it goes right

192
00:17:08,800 --> 00:17:14,120
the government doesn't create their war jets some big companies Lockheed Martin

193
00:17:14,120 --> 00:17:19,240
and all those make it so that's just how it works so



1
00:00:00,000 --> 00:00:04,520
Unfortunately, this is a reality of our lifetime,

2
00:00:04,520 --> 00:00:07,840
and it's another reason for us to have private,

3
00:00:07,840 --> 00:00:10,680
local AI, our own personal AI

4
00:00:10,680 --> 00:00:14,440
that we can use to combat all of this nonsense.

5
00:00:14,440 --> 00:00:19,520
We can use it to obfuscate stuff on the Internet,

6
00:00:19,520 --> 00:00:22,360
make us more private, these sorts of things.

7
00:00:22,360 --> 00:00:24,280
Because it's not just about,

8
00:00:24,280 --> 00:00:26,280
I have nothing to hide.

9
00:00:26,280 --> 00:00:29,400
The government hides stuff all the time.

10
00:00:29,400 --> 00:00:31,680
They have their secrets,

11
00:00:31,680 --> 00:00:37,360
and I have my personal life that I don't really want people to know,

12
00:00:37,360 --> 00:00:38,920
because then they can steal my identity,

13
00:00:38,920 --> 00:00:42,600
and they can verify a bunch of parts of my life.

14
00:00:42,600 --> 00:00:47,960
If you're collecting stuff in the government or any other corporation,

15
00:00:47,960 --> 00:00:51,040
mother's maiden name, father's middle name,

16
00:00:51,040 --> 00:00:53,720
favorite pet, you collect all that information.

17
00:00:53,720 --> 00:00:55,720
If somebody bad gets a hold of it,

18
00:00:55,720 --> 00:00:57,760
and they decide to impersonate me,

19
00:00:57,760 --> 00:01:00,320
and my bank says, what's your mother's maiden name?

20
00:01:00,320 --> 00:01:02,920
And they're just like, here it is.

21
00:01:02,920 --> 00:01:05,400
They can really ruin people's lives with that information,

22
00:01:05,400 --> 00:01:08,800
and that's why it's important to keep as much as possible

23
00:01:08,800 --> 00:01:10,640
about our lives private.

24
00:01:10,640 --> 00:01:13,000
That's the argument.

25
00:01:13,000 --> 00:01:15,560
You can't just say you have nothing to hide.

26
00:01:15,560 --> 00:01:18,840
That's the dumbest argument I've ever heard on the planet.

27
00:01:18,840 --> 00:01:20,480
If people argue that,

28
00:01:20,480 --> 00:01:25,080
you're now armed with a counter-argument.

29
00:01:25,080 --> 00:01:28,080
And that's a very valid counter-argument.

30
00:01:28,080 --> 00:01:30,520
Think about how much scamming is happening right now.

31
00:01:30,520 --> 00:01:33,560
So it's a very valid counter-argument.

32
00:01:33,560 --> 00:01:35,640
You don't have anything to hide, yes,

33
00:01:35,640 --> 00:01:37,800
but you should hide things,

34
00:01:37,800 --> 00:01:42,080
because there are people out there that want to abuse your public information,

35
00:01:42,080 --> 00:01:44,600
not just for ads, but to take your money,

36
00:01:44,600 --> 00:01:47,520
maybe take your homes, and put you, you know...

37
00:01:47,520 --> 00:01:50,720
I have known people that have had fraud happen to them,

38
00:01:50,720 --> 00:01:53,200
and it is devastating to their lives.

39
00:01:53,200 --> 00:01:55,720
It is very difficult to get out of it.

40
00:01:55,720 --> 00:01:58,360
They go and they open up bank accounts,

41
00:01:58,360 --> 00:02:00,360
they take all of your money from bank accounts,

42
00:02:00,360 --> 00:02:03,440
they then go to casinos and open up lines of credit,

43
00:02:03,440 --> 00:02:06,640
and it just completely destroys your credit.

44
00:02:06,640 --> 00:02:08,040
How are you going to buy a house with bad credit

45
00:02:08,040 --> 00:02:09,800
because somebody stole your identity?

46
00:02:09,800 --> 00:02:10,800
It's not unheard of.

47
00:02:10,800 --> 00:02:13,120
And the fact that I'm just one random person

48
00:02:13,120 --> 00:02:15,480
in the 300 million in the United States,

49
00:02:15,480 --> 00:02:20,720
and I have a close personal friend who has had to deal with this,

50
00:02:20,720 --> 00:02:23,200
and, you know, what are the odds?

51
00:02:23,200 --> 00:02:27,000
Pretty high, there's people out there now.

52
00:02:27,000 --> 00:02:33,160
Now, you know, it's becoming easier with how much data is being collected.

53
00:02:33,160 --> 00:02:36,280
A lot of that information could just be stolen from a big company

54
00:02:36,280 --> 00:02:39,280
that collected that information without telling you in the first place.

55
00:02:39,280 --> 00:02:43,600
So, government, surveillance, and AI, no-no.

56
00:02:43,600 --> 00:02:46,440
Don't even... if you hear about it, just look into it.

57
00:02:46,440 --> 00:02:48,120
Let's all rally together.

58
00:02:48,120 --> 00:02:50,280
It doesn't matter where you are on the political spectrum.

59
00:02:50,280 --> 00:02:51,720
I really don't care.

60
00:02:51,720 --> 00:02:53,320
But we all can get behind this, right?

61
00:02:53,320 --> 00:02:56,520
No government surveillance with AI, please.

62
00:02:56,520 --> 00:02:59,280
Sounds like a fantastic idea.

63
00:02:59,280 --> 00:03:02,400
And I'll leave it at that.

64
00:03:02,400 --> 00:03:05,680
Okay, so besides all of that,

65
00:03:05,680 --> 00:03:10,400
some of the tech stories that came out,

66
00:03:10,400 --> 00:03:13,480
there's SanDisk Extreme SSDs.

67
00:03:13,480 --> 00:03:15,720
I don't know if you heard about these.

68
00:03:15,720 --> 00:03:20,240
The 4-terabyte models, and I think the 16-terabyte models

69
00:03:20,240 --> 00:03:24,240
they're usually touted as some of the best, you know,

70
00:03:24,240 --> 00:03:26,120
because they're so small and thin.

71
00:03:26,120 --> 00:03:28,880
These tiny little drives that you can take with you

72
00:03:28,880 --> 00:03:30,880
have just been wiping people's data.

73
00:03:30,880 --> 00:03:33,200
It's just... just destroying it.

74
00:03:33,200 --> 00:03:35,480
Just willy-nilly wipe the whole drive.

75
00:03:35,480 --> 00:03:37,280
If you want to put it back on, sure, go ahead.

76
00:03:37,280 --> 00:03:39,560
And then it slowly wipes it.

77
00:03:39,560 --> 00:03:40,560
That's crazy.

78
00:03:40,560 --> 00:03:42,960
What's even crazier is it's not even faulty hardware.

79
00:03:42,960 --> 00:03:44,720
It's a faulty firmware.

80
00:03:44,720 --> 00:03:49,640
So, a firmware, I think they're updating the firmware as we speak.

81
00:03:49,640 --> 00:03:52,560
There's an article here,

82
00:03:52,560 --> 00:03:56,760
SanDisk to release firmware fix for failing Extreme

83
00:03:56,760 --> 00:03:58,960
and Extreme Pro V2 SSDs.

84
00:03:58,960 --> 00:04:02,720
And both of these articles are from today.

85
00:04:02,720 --> 00:04:04,760
So, one is like, it's broken.

86
00:04:04,760 --> 00:04:06,120
The other one's like, they're fixing it.

87
00:04:06,120 --> 00:04:09,200
So, yeah, I think it is a bit extreme

88
00:04:09,200 --> 00:04:15,400
if your little hard drive is wiping everything people put on it.

89
00:04:15,400 --> 00:04:17,680
The whole point of an external drive is

90
00:04:17,680 --> 00:04:20,520
for you to be able to put some stuff on there,

91
00:04:20,520 --> 00:04:21,920
transfer it somewhere else,

92
00:04:21,920 --> 00:04:23,680
keep it on there for a little while.

93
00:04:23,680 --> 00:04:25,400
I don't know.

94
00:04:25,400 --> 00:04:27,080
I would imagine if I put something on a drive,

95
00:04:27,080 --> 00:04:29,480
I'd want it to be there the next month.

96
00:04:29,480 --> 00:04:31,240
It's not like a thumbstick or something.

97
00:04:31,240 --> 00:04:34,720
So, yeah, be aware of that.

98
00:04:34,720 --> 00:04:40,480
If you have those, update your firmware

99
00:04:40,480 --> 00:04:45,240
and make sure that your data is still on your drives.

100
00:04:45,240 --> 00:04:54,920
Now, I think that some more stuff related to AI and tech

101
00:04:54,920 --> 00:04:58,560
is more of a moralistic problem.

102
00:04:58,560 --> 00:05:00,280
It's a training problem.

103
00:05:00,280 --> 00:05:04,720
So, we have, there's been this problem with YouTube,

104
00:05:04,720 --> 00:05:08,600
with Vimeo, with any web video,

105
00:05:08,600 --> 00:05:12,880
and also with text and stories and all this sort of stuff

106
00:05:12,880 --> 00:05:15,960
where somebody, there's a human somewhere

107
00:05:15,960 --> 00:05:19,960
that has to look at this stuff.

108
00:05:19,960 --> 00:05:22,720
And it could be mutilation and murder

109
00:05:22,720 --> 00:05:26,240
and all kinds of really bad things going on in these videos

110
00:05:26,240 --> 00:05:28,920
and these descriptions and text and stuff.

111
00:05:28,920 --> 00:05:32,440
It goes to the web that need to be filtered by a person.

112
00:05:32,440 --> 00:05:35,240
Now, of course, we have a lot of AI that goes in

113
00:05:35,240 --> 00:05:38,280
and detects a lot of that stuff,

114
00:05:38,280 --> 00:05:40,720
but somebody had to train it, the AI,

115
00:05:40,720 --> 00:05:43,440
by looking explicitly at all this stuff.

116
00:05:43,440 --> 00:05:46,280
And I think that a lot of companies

117
00:05:46,280 --> 00:05:49,400
outsource this to other countries

118
00:05:49,400 --> 00:05:52,680
just because it's cheaper.

119
00:05:52,680 --> 00:05:55,920
And frankly, that's basically the only reason, I assume.

120
00:05:55,920 --> 00:06:00,200
And these people are, they get traumatized.

121
00:06:00,200 --> 00:06:04,080
A lot of people, you know, there are a lot of people now

122
00:06:04,080 --> 00:06:06,240
with what kind of websites exist

123
00:06:06,240 --> 00:06:08,680
that traumatize themselves for enjoyment.

124
00:06:08,680 --> 00:06:10,840
But a lot of these people who just wanna make some money

125
00:06:10,840 --> 00:06:14,680
for their family, who wanna work hard and, you know,

126
00:06:14,680 --> 00:06:18,000
just provide, they have to do these kinds of jobs

127
00:06:18,000 --> 00:06:18,840
and it's traumatizing.

128
00:06:18,840 --> 00:06:20,280
They don't wanna be exposed to all this stuff.

129
00:06:20,280 --> 00:06:22,680
And it's a hard question.

130
00:06:22,680 --> 00:06:25,280
I brought this one up because I am training some AI stuff

131
00:06:25,280 --> 00:06:26,480
and it's a hard question.

132
00:06:26,480 --> 00:06:28,920
How do you avoid having to look at all this stuff,

133
00:06:28,920 --> 00:06:32,000
but also have to look at it in order to train things?

134
00:06:32,000 --> 00:06:36,200
Now, it's my hope that one day we'll have an AI model

135
00:06:36,200 --> 00:06:39,280
specifically trained on all of this nastiness

136
00:06:39,280 --> 00:06:41,160
so that it can deal with it.

137
00:06:41,160 --> 00:06:43,880
And that would be fantastic.

138
00:06:43,880 --> 00:06:47,480
And I think that we do have some that people are working on.

139
00:06:47,480 --> 00:06:51,560
Ironically, there's a bunch of NSFW kind of models

140
00:06:51,560 --> 00:06:54,040
people are training and putting out there.

141
00:06:54,040 --> 00:06:56,440
And those could be used potentially

142
00:06:56,440 --> 00:06:58,640
for detecting this kind of stuff.

143
00:06:58,640 --> 00:07:03,240
So I do wonder if there's a way that we can,

144
00:07:03,240 --> 00:07:07,040
like, you know that story where the person got a paperclip

145
00:07:07,040 --> 00:07:08,320
and they wound up with a house in the end

146
00:07:08,320 --> 00:07:10,080
because they traded the paperclip for a pencil

147
00:07:10,080 --> 00:07:13,400
and then traded the pencil for, I don't know,

148
00:07:13,400 --> 00:07:14,560
some other thing.

149
00:07:14,560 --> 00:07:15,440
It was never money.

150
00:07:15,440 --> 00:07:18,440
And then they wound up with a car, a house, something big,

151
00:07:18,440 --> 00:07:19,800
all from a paperclip.

152
00:07:19,800 --> 00:07:23,360
I feel like maybe we can make that paperclip AI

153
00:07:23,360 --> 00:07:27,040
and then kind of train it up in some way

154
00:07:27,040 --> 00:07:28,920
that it trains itself and it trains the next one

155
00:07:28,920 --> 00:07:29,760
and the next one trains it.

156
00:07:29,760 --> 00:07:31,040
And we kind of go back and forth

157
00:07:31,040 --> 00:07:33,360
with this adversarial kind of training.

158
00:07:34,480 --> 00:07:38,080
That way, we don't have to expose ourselves to all this.

159
00:07:38,080 --> 00:07:39,600
It's an interesting moral question.

160
00:07:39,600 --> 00:07:42,320
I am curious to see what people think about how to solve it

161
00:07:42,320 --> 00:07:43,960
because I don't think we should expose people

162
00:07:43,960 --> 00:07:45,000
to all this stuff.

163
00:07:45,000 --> 00:07:48,080
But at the same time, some people,

164
00:07:48,080 --> 00:07:49,800
there will have to be a human element to it

165
00:07:49,800 --> 00:07:51,520
and somebody has to make the sacrifice.

166
00:07:51,520 --> 00:07:54,400
And of course, there's always been people

167
00:07:54,400 --> 00:07:57,040
throughout all of history that have made

168
00:07:57,040 --> 00:07:58,960
a mental and physical sacrifice

169
00:07:58,960 --> 00:08:01,480
for the betterment of the people that come after

170
00:08:01,480 --> 00:08:02,600
for their children.

171
00:08:02,600 --> 00:08:07,160
And so it could be the cost and somebody's got to pay it.

172
00:08:07,160 --> 00:08:08,880
It's an interesting problem to solve.

173
00:08:08,880 --> 00:08:12,240
I think if somebody can accelerate solving that

174
00:08:12,240 --> 00:08:15,000
so that we can get people off of manually training

175
00:08:15,000 --> 00:08:18,680
all these models on this really bad stuff

176
00:08:18,680 --> 00:08:21,880
that just shouldn't exist, the better.

177
00:08:23,960 --> 00:08:28,720
So that's a lot of questions I have on that.

178
00:08:28,720 --> 00:08:33,720
But now we're going to move on to security stuff.

179
00:08:34,920 --> 00:08:39,000
I have some security updates for people,

180
00:08:39,000 --> 00:08:43,160
but I also have some security warnings for people.

181
00:08:43,160 --> 00:08:45,720
We already talked about people's secure data

182
00:08:45,720 --> 00:08:50,200
and transferring it to big corporations.

183
00:08:50,200 --> 00:08:52,240
So why not add in some more?

184
00:08:52,240 --> 00:08:55,480
Amazon has now some palm scanning technology

185
00:08:55,480 --> 00:09:00,160
that lets you buy a drink or groceries or anything else.

186
00:09:00,160 --> 00:09:03,760
You scan your palm and it purchases stuff

187
00:09:03,760 --> 00:09:07,880
probably using Amazon Pay or verifies your identity

188
00:09:07,880 --> 00:09:11,800
and it also verifies your age and all that sort of stuff

189
00:09:11,800 --> 00:09:14,920
if you want to buy things that are age restricted.

190
00:09:14,920 --> 00:09:17,880
And so



1
00:00:00,000 --> 00:00:06,240
Amazon itself, it's kind of not surprising that they move into that.

2
00:00:06,240 --> 00:00:10,700
They're a purchasing company where you buy commodities,

3
00:00:10,700 --> 00:00:15,080
and they're also a tech company with AWS and all their server stuff.

4
00:00:15,080 --> 00:00:24,040
So it makes sense that they would continue to try and combine tech and payments.

5
00:00:24,040 --> 00:00:27,760
So obviously, I don't even have to say it,

6
00:00:27,760 --> 00:00:33,360
though the big problem with this is you're associating your ID

7
00:00:33,360 --> 00:00:38,440
and your now physical body even more with corporations.

8
00:00:38,440 --> 00:00:42,280
And a lot of biometric tech is local, right?

9
00:00:42,280 --> 00:00:48,080
So you have IR cameras on your laptop for Windows Hello.

10
00:00:48,080 --> 00:00:50,800
Who knows, they probably store it in some server somewhere.

11
00:00:50,800 --> 00:00:55,560
But you can unlock your computer without an internet connection using it.

12
00:00:55,560 --> 00:01:00,520
So these biometric things exist,

13
00:01:00,520 --> 00:01:07,360
but now we're starting to talk about using them for purchases with corporations.

14
00:01:07,360 --> 00:01:09,280
And that gets kind of weird.

15
00:01:09,280 --> 00:01:11,760
I feel like they already have enough data,

16
00:01:11,760 --> 00:01:18,200
but now they want your palm friends, your iris scans, your facial recognition,

17
00:01:18,200 --> 00:01:22,160
your ID cards, they want your toe print at some point.

18
00:01:22,160 --> 00:01:25,360
And so it's getting a little much.

19
00:01:25,360 --> 00:01:29,360
And I'd like to see some pushback on this sort of stuff.

20
00:01:29,360 --> 00:01:34,320
I want people to say, no, I'm not going to scan my palm.

21
00:01:34,320 --> 00:01:37,360
I mean, what's the difference between scanning your palm

22
00:01:37,360 --> 00:01:40,320
and just taking out your phone and scanning your phone

23
00:01:40,320 --> 00:01:46,080
whenever you use your card NFC reader?

24
00:01:46,080 --> 00:01:48,800
So really, I don't see the advantage here.

25
00:01:48,800 --> 00:01:51,360
It's not like it makes things that much faster.

26
00:01:51,360 --> 00:01:53,640
Most people already have their phones in their hands,

27
00:01:53,640 --> 00:01:57,000
so why not just bloop?

28
00:01:57,000 --> 00:02:00,280
I mean, is it really that hard?

29
00:02:00,280 --> 00:02:04,800
Is it really that hard to pull your wallet out and scan a card

30
00:02:04,800 --> 00:02:08,280
for a huge advantage in privacy?

31
00:02:08,280 --> 00:02:10,120
It seems kind of weird to me.

32
00:02:10,120 --> 00:02:13,960
And I hope this isn't the way that everyone kind of pushes for,

33
00:02:13,960 --> 00:02:18,160
is more of this biometric identification.

34
00:02:18,160 --> 00:02:21,040
I don't want to one day walk down the street

35
00:02:21,040 --> 00:02:25,640
and then some kind of billboard scans my face

36
00:02:25,640 --> 00:02:30,000
and realizes who I am and then swaps to an advertisement for me.

37
00:02:30,000 --> 00:02:33,800
And then they'll be like, oh, it's super private.

38
00:02:33,800 --> 00:02:38,720
Only at your viewing angle can you see these pixels that are designed.

39
00:02:38,720 --> 00:02:41,280
Everyone sees their own unique billboard.

40
00:02:41,280 --> 00:02:42,560
I don't know.

41
00:02:42,560 --> 00:02:44,960
You can guarantee that's going to be some kind of nonsense

42
00:02:44,960 --> 00:02:46,000
that's going to come up.

43
00:02:46,000 --> 00:02:51,080
Yeah, I don't want to integrate my presence

44
00:02:51,080 --> 00:02:55,120
into corporations and all that sort of stuff.

45
00:02:55,120 --> 00:03:00,200
I'm okay with all of this disappearing once I get put six feet under.

46
00:03:00,200 --> 00:03:03,440
So it's fine with me.

47
00:03:03,440 --> 00:03:06,400
But yeah, a little bit of security there.

48
00:03:06,400 --> 00:03:10,520
If you've seen these things at wherever Amazon is placing these,

49
00:03:10,520 --> 00:03:15,240
just say no and pull out your ID or your card or whatever.

50
00:03:15,240 --> 00:03:20,440
It's a good idea just to say no sometimes.

51
00:03:20,440 --> 00:03:25,040
A little PSA for people who have Samsung phones,

52
00:03:25,040 --> 00:03:28,280
like I do, I have three different variants

53
00:03:28,280 --> 00:03:30,080
just right here on this desk.

54
00:03:30,080 --> 00:03:31,480
I'm not a fanboy or anything.

55
00:03:31,480 --> 00:03:37,520
I just like Android phones and Samsung has nice phones.

56
00:03:37,520 --> 00:03:43,480
So anyways, it affects Samsung devices running

57
00:03:43,480 --> 00:03:47,160
for Android versions 11, 12, and 13.

58
00:03:47,160 --> 00:03:49,280
There's a...

59
00:03:49,280 --> 00:03:52,080
Okay, so when you load up an app on your phone

60
00:03:52,080 --> 00:03:57,160
and on some computers, or I guess most computers,

61
00:03:57,160 --> 00:04:03,000
it'll take that application and it will scramble it

62
00:04:03,000 --> 00:04:05,240
in a way that you can't figure out where it's at.

63
00:04:05,240 --> 00:04:07,840
So traditionally on old school computers,

64
00:04:07,840 --> 00:04:12,040
your program would load and it would be put into a part of your RAM

65
00:04:12,040 --> 00:04:16,200
and it would read instruction by instruction.

66
00:04:16,200 --> 00:04:19,120
And anything that knows where that program is

67
00:04:19,120 --> 00:04:21,560
can actually alter it and mess with it.

68
00:04:21,560 --> 00:04:25,720
So they had to create ways of preventing access

69
00:04:25,720 --> 00:04:29,640
from other programs, whether it's permissions or security,

70
00:04:29,640 --> 00:04:33,320
but also through obfuscation where it loads up your program

71
00:04:33,320 --> 00:04:35,120
and it scrambles where it's at.

72
00:04:35,120 --> 00:04:37,720
Maybe it separates it out into different parts of memory

73
00:04:37,720 --> 00:04:43,640
or it has an addressing, this is where the app is,

74
00:04:43,640 --> 00:04:45,680
but it goes through a layer that reroutes it,

75
00:04:45,680 --> 00:04:47,080
that reroutes to something else.

76
00:04:47,080 --> 00:04:49,480
And so it's hard to figure out where the app is.

77
00:04:49,480 --> 00:04:53,320
So there's a bug on the Samsung devices on 11, 12, and 13

78
00:04:53,320 --> 00:04:57,280
that doesn't do that rerouting for the application.

79
00:04:57,280 --> 00:05:01,400
So this is not a big security risk for a lot of people.

80
00:05:01,400 --> 00:05:03,760
It's just keep your phone updated.

81
00:05:03,760 --> 00:05:04,920
Let me see when this was.

82
00:05:04,920 --> 00:05:07,480
May 19th, so keep your phone updated.

83
00:05:07,480 --> 00:05:13,320
I think they'll be putting out a update in June, I think it said.

84
00:05:13,320 --> 00:05:16,600
June 9th is when there are expected to be patches

85
00:05:16,600 --> 00:05:18,520
or they'd hope for patches.

86
00:05:18,520 --> 00:05:21,640
So just look out for an update, keep your phone up to date.

87
00:05:21,640 --> 00:05:24,680
Not super critical of a bug.

88
00:05:24,680 --> 00:05:26,960
You have to already be infected with something for it

89
00:05:26,960 --> 00:05:27,800
to abuse this.

90
00:05:27,800 --> 00:05:33,000
So not likely a target vector many people would,

91
00:05:33,000 --> 00:05:37,520
many hackers would aim for.

92
00:05:37,520 --> 00:05:40,040
Another thing in security that happened recently

93
00:05:40,040 --> 00:05:42,120
is PyPi repositories.

94
00:05:42,120 --> 00:05:43,580
And if you don't know what this is,

95
00:05:43,580 --> 00:05:45,600
people use Python programming language.

96
00:05:45,600 --> 00:05:48,440
And if you use any language, like C Sharp and NuGet

97
00:05:48,440 --> 00:05:54,120
or whatever, any kind of package manager for programmers,

98
00:05:54,120 --> 00:05:56,200
we have this problem where programmers are lazy.

99
00:05:56,200 --> 00:05:58,040
And so they just download a bunch of packages

100
00:05:58,040 --> 00:05:59,120
to do stuff for them.

101
00:05:59,120 --> 00:06:00,960
They'll read the description and be like, ah, that's great.

102
00:06:00,960 --> 00:06:02,800
Download it and start using it.

103
00:06:02,800 --> 00:06:05,840
And these packages do take time for people

104
00:06:05,840 --> 00:06:08,560
to audit their security.

105
00:06:08,560 --> 00:06:11,840
And they use algorithms to find common ones.

106
00:06:11,840 --> 00:06:14,480
But if somebody just programmed up one and put it up

107
00:06:14,480 --> 00:06:19,920
as a package, it could take some time before they are found.

108
00:06:19,920 --> 00:06:26,160
And so this repository for PyPi, our many repositories

109
00:06:26,160 --> 00:06:29,280
that were kind of linked to by PyPi,

110
00:06:29,280 --> 00:06:32,440
started having all kinds of malicious hardware.

111
00:06:32,440 --> 00:06:35,720
They are malicious software, malicious packages

112
00:06:35,720 --> 00:06:38,400
or whatever you want to call them, were being uploaded.

113
00:06:38,400 --> 00:06:40,080
And people were downloading these.

114
00:06:40,080 --> 00:06:44,960
One of them was downloaded 1,200 times before it got caught.

115
00:06:44,960 --> 00:06:48,080
And PyPi actually had to shut down signups and everything

116
00:06:48,080 --> 00:06:50,720
else over the weekend for 17 hours

117
00:06:50,720 --> 00:06:52,440
just to clean all this crap up.

118
00:06:52,440 --> 00:06:56,040
So don't download packages unless you know it's good

119
00:06:56,040 --> 00:06:58,040
or you audited it yourself or it's

120
00:06:58,040 --> 00:07:01,120
been audited by a security professional or someone

121
00:07:01,120 --> 00:07:02,000
you trust.

122
00:07:02,000 --> 00:07:05,560
So it's not that hard to write some code.

123
00:07:05,560 --> 00:07:07,280
Just don't just download anything.

124
00:07:07,280 --> 00:07:09,640
There are a couple of examples where

125
00:07:09,640 --> 00:07:13,440
there was a package named Node.js Encrypt Agent and Node.js

126
00:07:13,440 --> 00:07:18,240
Cookie Proxy Agent, which is a funny Node.js thing in Python.

127
00:07:18,240 --> 00:07:20,400
And that was the one that was downloaded 1,200 times

128
00:07:20,400 --> 00:07:22,560
before it got caught.

129
00:07:22,560 --> 00:07:24,600
And then in March this year, there

130
00:07:24,600 --> 00:07:29,840
was another malicious package called Colorful with a U.

131
00:07:29,840 --> 00:07:33,440
Colorful on PyPi.

132
00:07:33,440 --> 00:07:37,640
It was discovered being distributed.

133
00:07:37,640 --> 00:07:40,680
It was malware.

134
00:07:40,680 --> 00:07:43,000
It was referred to as colorblind.

135
00:07:43,000 --> 00:07:46,680
I guess they were using, what's it called,

136
00:07:46,680 --> 00:07:50,360
some accessibility verbiage.

137
00:07:50,360 --> 00:07:53,720
And people were just downloading it.

138
00:07:53,720 --> 00:07:57,120
So yeah, if you are a programmer and you're out there programming

139
00:07:57,120 --> 00:08:00,680
away, don't just download packages.

140
00:08:00,680 --> 00:08:02,960
Use vetted ones or write it yourself.

141
00:08:02,960 --> 00:08:04,800
It's not that hard to write your own code.

142
00:08:04,800 --> 00:08:06,440
A lot of these, we're targeting people

143
00:08:06,440 --> 00:08:08,720
to try and copy from their clipboard

144
00:08:08,720 --> 00:08:12,840
in order to get cryptocurrency wallets and that sort of stuff.

145
00:08:12,840 --> 00:08:15,560
Cryptocurrency is a big vector for people stealing stuff

146
00:08:15,560 --> 00:08:17,800
because, poof, it's gone.

147
00:08:17,800 --> 00:08:18,800
Who's going to trace it?

148
00:08:18,800 --> 00:08:20,000
Nobody's going to know.

149
00:08:20,000 --> 00:08:23,080
So yeah, that's just an update for programmers

150
00:08:23,080 --> 00:08:26,640
to keep an eye out for.

151
00:08:26,640 --> 00:08:34,160
And on the coding front, I just discovered

152
00:08:34,160 --> 00:08:36,960
people use online coding.

153
00:08:36,960 --> 00:08:40,040
Am I so old that I'm the offline coding guy?

154
00:08:40,040 --> 00:08:44,880
I'm the guy that uses dinosaur code or something?

155
00:08:44,880 --> 00:08:48,000
There's articles about online coding

156
00:08:48,000 --> 00:08:49,960
and the advantages of it.

157
00:08:49,960 --> 00:08:51,800
And then there's counter articles

158
00:08:51,800 --> 00:08:54,360
about why you should use offline coding.

159
00:08:54,360 --> 00:08:56,760
And this was the first I've ever heard of this.

160
00:08:56,760 --> 00:08:58,680
I get that you can use the Visual Studio

161
00:08:58,680 --> 00:09:02,120
code, basically, inside of GitHub or whatever.

162
00:09:02,120 --> 00:09:03,400
You could pay for their thing.

163
00:09:03,400 --> 00:09:04,520
I don't know why you would.

164
00:09:04,520 --> 00:09:06,800
I mean, you've got to use a computer to access it.

165
00:09:06,800 --> 00:09:08,120
Your computer can't be that bad.

166
00:09:08,120 --> 00:09:10,360
No, it can't run Visual Studio code.

167
00:09:13,240 --> 00:09:15,080
But apparently, this is the thing.

168
00:09:15,080 --> 00:09:18,360
Online coding.

169
00:09:18,360 --> 00:09:21,600
If you are an online coder, tell me what that means.

170
00:09:21,600 --> 00:09:26,160
I read these articles, and I don't fully understand it.

171
00:09:26,160 --> 00:09:30,160
Some of the arguments were offline coding is more secure.

172
00:09:30,160 --> 00:09:33,040
You don't have to worry about internet distractions.

173
00:09:33,040 --> 00:09:39,440
It seems like it seems kind of, you know, I don't know.

174
00:09:39,440 --> 00:09:42,360
Maybe new coders, coders who are learning to program

175
00:09:42,360 --> 00:09:44,120
or coming from another field, people

176
00:09:44,120 --> 00:09:47,600
who haven't drudged through the mud

177
00:09:47,600 --> 00:09:53,160
and the sludge of programming to really say

178
00:09:53,160 --> 00:09:58,080
that they have to use offline systems for development.

179
00:09:58,080 --> 00:10:00,320
Maybe it's those people.

180
00:10:00,320 --> 00:10:01,800
Maybe it's web developers.

181
00:10:01,800 --> 00:10:03,760
It's just a lot easier to do stuff on the web.

182
00:10:03,760 --> 00:10:05,640
I don't know.

183
00:10:05,640 --> 00:10:07,640
But yeah, use offline coding.

184
00:10:07,640 --> 00:10:09,840
In fact, turn off your internet when you code.

185
00:10:09,840 --> 00:10:10,880
Just code.

186
00:10:10,880 --> 00:10:13,640
Have some, just breathe.

187
00:10:13,640 --> 00:10:16,480
Drink some tea or whatever it is you drink.

188
00:10:16,480 --> 00:10:21,120
Soda, Coke Zero, and relax and do some code.

189
00:10:21,120 --> 00:10:23,200
It's therapeutic.

190
00:10:23,200 --> 00:10:25,640
You don't need the internet to code.

191
00:10:25,640 --> 00:10:27,200
I know that you do like Stack Overflow.

192
00:10:27,200 --> 00:10:31,600
And of course, now you like your GitHub Copilot.

193
00:10:31,600 --> 00:10:33,200
I mean, I like GitHub Copilot.

194
00:10:33,200 --> 00:10:38,200
I use it for just fooling around and learning languages

195
00:10:38,200 --> 00:10:38,680
and stuff.

196
00:10:38,680 --> 00:10:41,440
So I mean, I get that appeal.

197
00:10:41,440 --> 00:10:44,800
But I've spent most of my time coding essentially

198
00:10:44,800 --> 00:10:46,120
without an internet connection.

199
00:10:46,120 --> 00:10:48,280
So it's very nice.

200
00:10:48,280 --> 00:10:53,920
So that's about it that I have for today.

201
00:10:53,920 --> 00:10:56,080
Not a whole lot of stuff.

202
00:10:56,080 --> 00:10:58,280
But it was a lot.

203
00:10:58,280 --> 00:11:01,080
I mean, it's kind of the stuff that happened recently.

204
00:11:01,080 --> 00:11:02,800
And there's a lot of security stuff.

205
00:11:02,800 --> 00:11:04,300
Just pay attention to your security

206
00:11:04,300 --> 00:11:05,720
and pay attention to your privacy.

207
00:11:05,720 --> 00:11:10,720
Use apps like Signal to talk to your family and friends.

208
00:11:10,720 --> 00:11:14,960
Just don't even log into Facebook, that sort of stuff.

209
00:11:14,960 --> 00:11:16,160
And have a good one.

210
00:11:16,160 --> 00:11:40,960
So bye for now.



