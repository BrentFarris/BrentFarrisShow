1
00:00:00,000 --> 00:00:04,440
Welcome back! Not a whole lot of tech news over the weekend or Monday. I meant

2
00:00:04,440 --> 00:00:08,560
to put out a video on Monday but I was sidetracked training AI and learning Go.

3
00:00:08,560 --> 00:00:17,600
But 1.3 billion dollars is how much Facebook is being fined. Which I think

4
00:00:17,600 --> 00:00:25,480
they made 23 billion dollars like last year or maybe this year so far. I'm not

5
00:00:25,480 --> 00:00:32,960
sure. Oh 28 billion. 28 billion in the first three months of this year. So yes

6
00:00:32,960 --> 00:00:38,000
they've made 28 billion and they're being fined 1.3 billion. Of course 1.3

7
00:00:38,000 --> 00:00:43,000
billion is probably the GDP of some small country. So it's a lot of money for

8
00:00:43,000 --> 00:00:49,240
somebody but not a lot of money for Facebook. Now why are they being fined

9
00:00:49,240 --> 00:00:55,920
over a billion dollars? Well they're being fined because they were transmitting

10
00:00:55,920 --> 00:01:06,520
data from the EU into the United States. Arguably the one of the at least top

11
00:01:06,520 --> 00:01:11,360
five freest countries in the world. I know I know. But at the same time they

12
00:01:11,360 --> 00:01:15,240
are big corporations and I am a huge advocate of not giving your your

13
00:01:15,240 --> 00:01:22,000
information to big corporations and also not giving them basically everything

14
00:01:22,000 --> 00:01:26,880
about your life. Now whether you choose to do that willingly is a different

15
00:01:26,880 --> 00:01:31,080
question. A lot of people do use these services and upload their photos and

16
00:01:31,080 --> 00:01:36,520
include geolocation and say that I'm okay whenever some natural disaster

17
00:01:36,520 --> 00:01:41,200
happens and says and they do check-ins where they check into some place they've

18
00:01:41,200 --> 00:01:46,760
been. So they they have to know. People do have to know that that data is going out

19
00:01:46,760 --> 00:01:55,240
to somebody like their friends. They're posting it to somewhere. Now I should I

20
00:01:55,240 --> 00:02:00,600
should think that everything should have a little bit of security to it but our

21
00:02:00,600 --> 00:02:08,160
privacy is a commodity to big companies and advertising agencies and AI training

22
00:02:08,160 --> 00:02:12,720
people. People who train AI are definitely gonna be the new the new

23
00:02:12,720 --> 00:02:16,760
people who want your data by the way and Facebook is one of those people

24
00:02:16,760 --> 00:02:24,600
training AI. In fact PyTorch one of the top two AI training frameworks is

25
00:02:24,600 --> 00:02:30,280
developed by Facebook. In fact it's it's competing for number one spot with like

26
00:02:30,280 --> 00:02:37,360
TensorFlow and they're constantly in flux with each other. So yeah data is is

27
00:02:37,360 --> 00:02:45,880
a big thing and so the EU is is charging Facebook this 1.3 billion dollars

28
00:02:45,880 --> 00:02:51,400
because it accuses it of violating privacy rules and snooping. Now of course

29
00:02:51,400 --> 00:02:54,320
these companies have developed their architecture to not really care about

30
00:02:54,320 --> 00:02:58,000
what country you're in and they don't block off stuff you know unless it's in

31
00:02:58,000 --> 00:03:03,040
China. They do really good job of filtering stuff real quick there.

32
00:03:03,040 --> 00:03:09,520
Ultimately I'm not you know I'm not super up in arms about this. I do think

33
00:03:09,520 --> 00:03:17,960
that company that that countries finding companies is a way of tax or something

34
00:03:17,960 --> 00:03:21,040
like that. I don't I don't see what they're trying to get at here because

35
00:03:21,040 --> 00:03:25,680
let me let me just give you a little antidote about these big corporations. I

36
00:03:25,680 --> 00:03:33,720
worked at a pretty big game company in the past and there was a problem where

37
00:03:33,720 --> 00:03:39,440
people were using old iPads and there was a pretty significant amount of

38
00:03:39,440 --> 00:03:45,440
people using these old iPads for the application that they were working on

39
00:03:45,440 --> 00:03:51,960
and or that the application the game that we produced there there's tons of

40
00:03:51,960 --> 00:03:55,200
people on this and so they were valuable we didn't want to just cut them off

41
00:03:55,200 --> 00:04:00,480
of being able to play. We wanted those people to continue playing because it

42
00:04:00,480 --> 00:04:06,640
they generated revenue. Now the cost to developers to continue to try and

43
00:04:06,640 --> 00:04:12,920
support these old iPads was pretty expensive. So one of the talks that the

44
00:04:12,920 --> 00:04:19,400
company went about in was just sending every single customer on that version

45
00:04:19,400 --> 00:04:23,880
iPad a new iPad. They were just gonna buy a new iPad for all of the customers that

46
00:04:23,880 --> 00:04:30,440
played the game using that old model and the reason this was a discussion is

47
00:04:30,440 --> 00:04:35,720
because the one of the companies that was also equally large some employees

48
00:04:35,720 --> 00:04:40,440
came from did this exact thing and it saved them on development costs and all

49
00:04:40,440 --> 00:04:44,920
this sort of stuff. Why is this important to the Facebook thing? 1.3 billion

50
00:04:44,920 --> 00:04:50,320
dollars for being able to have access to all the information about people in the

51
00:04:50,320 --> 00:04:58,280
EU? It's just what they would consider a cost of doing business. So I can see that

52
00:04:58,280 --> 00:05:05,880
that the value to them to be able to keep this this data transfer going and

53
00:05:05,880 --> 00:05:13,280
not segment their platform is it's worth the cost. So they're not this this 1.3

54
00:05:13,280 --> 00:05:16,360
billion dollars isn't going to make them ultimately change how they do

55
00:05:16,360 --> 00:05:20,080
everything. Of course legislation will if they try to isolate all the

56
00:05:20,080 --> 00:05:25,960
servers in Europe and I would argue that's a good way to close

57
00:05:25,960 --> 00:05:31,240
off the internet and yes I know we're dealing with corporations but what if I

58
00:05:31,240 --> 00:05:38,280
a humble American in freedom land decided that I wanted to set up a

59
00:05:38,280 --> 00:05:43,200
company that was worldwide some language learning thing or I don't know some

60
00:05:43,200 --> 00:05:47,360
something to do your taxes. I guess tax is a bad thing because it's kind of

61
00:05:47,360 --> 00:05:55,920
regional. Something I wanted to sell a game and my game had game servers. Now I

62
00:05:55,920 --> 00:06:00,000
do want to set up a server over in the EU so that it's faster but I want to

63
00:06:00,000 --> 00:06:03,920
store all the data collectively in the United States with all my other data. I

64
00:06:03,920 --> 00:06:08,400
want to aggregate it into a central location so it's faster for processing

65
00:06:08,400 --> 00:06:13,680
and searching. So if somebody wants to search for some result like they want to

66
00:06:13,680 --> 00:06:16,920
look for a player on the other side of the planet they don't have to do a

67
00:06:16,920 --> 00:06:20,160
round-trip way over there it's all in one place and it's locally cached.

68
00:06:20,160 --> 00:06:26,920
Anyways semantics aside it would be very difficult for me to kind of avoid that

69
00:06:26,920 --> 00:06:32,040
law. It kind of makes the big bigger. It makes all the little people who want to

70
00:06:32,040 --> 00:06:37,320
start up a business be cut out of the business because now they can't serve

71
00:06:37,320 --> 00:06:43,840
anyone in Europe. It seems like you know big companies like you know Facebook and

72
00:06:43,840 --> 00:06:49,320
Google and Microsoft and all of them will easily be able to to manage all of

73
00:06:49,320 --> 00:06:53,520
that bureaucracy but the little guys won't. And that's usually how this

74
00:06:53,520 --> 00:06:57,880
legislation for these kinds of things go is it destroys the opportunity for small

75
00:06:57,880 --> 00:07:03,680
companies and it doesn't hurt these big guys. They just get huge and they just it

76
00:07:03,680 --> 00:07:06,480
just cuts out the competition for them to have to fight these battles. And of

77
00:07:06,480 --> 00:07:11,360
course they they just litigate you know and they do appeals and it takes forever

78
00:07:11,360 --> 00:07:16,080
for them to pay and they wind up paying less and all this other stuff. So I don't

79
00:07:16,080 --> 00:07:19,800
really see the value in this kind of thing. So that was one of the things that

80
00:07:19,800 --> 00:07:27,840
came up. And speaking of overseas servers and overseas data I also read that

81
00:07:27,840 --> 00:07:35,040
there is a leaked document from from from Europe again. I love I love my

82
00:07:35,040 --> 00:07:40,360
cousins in the Europe but you know some of the legislation is it's kind of crazy.

83
00:07:40,360 --> 00:07:48,400
There was a document about how a lot of lawmakers want to ban end-to-end

84
00:07:48,400 --> 00:07:53,360
encryption. They want to keep it but they want to ban it. They want to be able to

85
00:07:53,360 --> 00:07:58,520
read everybody's stuff to make sure they're not doing anything bad but they

86
00:07:58,520 --> 00:08:03,360
also want it to stay secure. It feels like these people have no idea what

87
00:08:03,360 --> 00:08:08,160
technology is. They've used a phone and they think they're experts or something.

88
00:08:08,160 --> 00:08:13,480
The article was it was specifically pointing at Spain because Spain really

89
00:08:13,480 --> 00:08:17,920
wanted to just ban end-to-end encryption totally and this was a leaked document

90
00:08:17,920 --> 00:08:23,560
or something like that. But there is it talked about how many members of the EU

91
00:08:23,560 --> 00:08:29,760
were doing proposals. So it says it reveals a strong strong support among

92
00:08:29,760 --> 00:08:35,840
EU member states for proposals to scan private messages for illegal content.

93
00:08:35,840 --> 00:08:42,880
Another part once this right here this is it's like a wolf in sheep's

94
00:08:42,880 --> 00:08:48,280
clothing. Craft a highly controversial law to stop the spread of child sexual

95
00:08:48,280 --> 00:08:54,240
abuse material in Europe. Now if Europe has a problem with child sexual abuse as

96
00:08:54,240 --> 00:08:59,600
every country in the world does they should definitely go to the fullest

97
00:08:59,600 --> 00:09:03,600
extensive law put those people in jail for life. I don't care what happens to

98
00:09:03,600 --> 00:09:11,280
those people. People who abuse children can rot for all I care. But this is often

99
00:09:11,280 --> 00:09:16,480
a tactic that they used. You'll remember I iPhone did this. They they started

100
00:09:16,480 --> 00:09:20,320
scanning people's libraries. If you had an iPhone I don't know if

101
00:09:20,320 --> 00:09:23,840
they still do it. I think they probably still do it. They scan all your photos

102
00:09:23,840 --> 00:09:27,320
and then report you to somebody if you have any kind of suspect photos. And

103
00:09:27,320 --> 00:09:31,720
their machine learning algorithms are horrid. There's all kinds of

104
00:09:31,720 --> 00:09:35,920
articles that came out when they launched it about people who would take

105
00:09:35,920 --> 00:09:40,600
pictures of their kids who were I don't know they fell off a slide or something

106
00:09:40,600 --> 00:09:44,760
and they were taking a picture to send to their doctor and they were flagged

107
00:09:44,760 --> 00:09:48,480
and the police showed up there at their door saying that they were child abusers.

108
00:09:48,480 --> 00:09:56,320
And so first of all it doesn't even work and second of all I don't think this is

109
00:09:56,320 --> 00:10:00,520
the most effective way to catch them. I think that it's a an excuse. It's kind of

110
00:10:00,520 --> 00:10:04,120
like how politicians just make up crap. They make up random titles that have

111
00:10:04,120 --> 00:10:07,880
nothing to do with bills in order to make everyone kind of get on the

112
00:10:07,880 --> 00:10:11,840
bandwagon with the name of the bill. It's the same kind of thing. Oh we want to

113
00:10:11,840 --> 00:10:15,200
create technology to scan to scan people's phones just to make sure there

114
00:10:15,200 --> 00:10:23,120
isn't or any kind of child abuse. And that's not exactly real. I don't

115
00:10:23,120 --> 00:10:27,200
think that's what it's for. I think it's a backdoor into people's private lives

116
00:10:27,200 --> 00:10:30,120
so that you can get more advertising money so that artificial intelligence

117
00:10:30,120 --> 00:10:35,920
can be better trained and so on and so forth. And you're using these presumably

118
00:10:35,920 --> 00:10:43,080
what would be high morals, these morals that anyone could agree to, but you're

119
00:10:43,080 --> 00:10:50,840
kind of using it as a facade to what you're actually doing. So there's another

120
00:10:50,840 --> 00:10:54,920
section in here that I highlighted. It said for years EU states have debated

121
00:10:54,920 --> 00:10:58,480
whether end-to-end encryption communication platforms such as WhatsApp

122
00:10:58,480 --> 00:11:03,840
and Signal should be protected as a way for Europeans to

123
00:11:03,840 --> 00:11:09,760
exercise a fundamental right to privacy or weakened to keep criminals from being

124
00:11:09,760 --> 00:11:16,080
able to communicate outside of the reach of the law enforcement. Now I get it.

125
00:11:16,080 --> 00:11:18,480
There's bad people everywhere and they're gonna find a way to talk. They'll

126
00:11:18,480 --> 00:11:21,800
use carrier pigeons and what are you gonna do? Scan the birds in the air? I

127
00:11:21,800 --> 00:11:26,960
mean with drones you probably already are. But I don't know. I'm a big person

128
00:11:26,960 --> 00:11:31,400
for privacy and security. For example when it comes to AI the reason I like

129
00:11:31,400 --> 00:11:36,400
I'm interested in AI and I work with AI and I train AI models now is because I'm

130
00:11:36,400 --> 00:11:42,620
interested in a private personal artificial intelligence that is not run

131
00:11:42,620 --> 00:11:46,640
by a company. Any company. It doesn't matter what if their name is OpenAI or

132
00:11:46,640 --> 00:11:51,480
not. Open sounds open but you know when you have tens of billions of dollars

133
00:11:51,480 --> 00:11:58,080
coming in from Microsoft you kind of you are gonna lean in one direction. So you

134
00:11:58,080 --> 00:12:03,960
know share a little too much information. So I am for all for personal encryption

135
00:12:03,960 --> 00:12:07,560
and yes there is a cost to it. There's a cost to freedom. There's always a cost to

136
00:12:07,560 --> 00:12:11,800
freedom. We all know it. We all can read our history books. There's always a cost

137
00:12:11,800 --> 00:12:18,880
to freedom and you know you can't you can't have both. Let's talk a little bit

138
00:12:18,880 --> 00:12:23,280
about government surveillance. This is something that has been coming up

139
00:12:23,280 --> 00:12:29,400
recently. It's a thing about governments wanting to use

140
00:12:29,400 --> 00:12:36,400
artificial intelligence to not exactly track people but to track people and you

141
00:12:36,400 --> 00:12:39,720
know predict things like when car crashes are going to happen or when

142
00:12:39,720 --> 00:12:44,760
somebody's doing something bad or whatever. Of course this is dystopian and

143
00:12:44,760 --> 00:12:51,000
everyone knows the references to what this is like but it's not just communist

144
00:12:51,000 --> 00:12:55,480
countries like China that are that are trying to impose these. It is freedom

145
00:12:55,480 --> 00:13:01,920
countries like Europe and even parts of the United States that want to monitor

146
00:13:01,920 --> 00:13:10,920
people. Now of course every government in world history is afraid of its

147
00:13:10,920 --> 00:13:15,280
own people in some way or another. The biggest danger is the danger

148
00:13:15,280 --> 00:13:20,960
within and you can see this in a lot of wars nobody at least recently nobody has

149
00:13:20,960 --> 00:13:24,720
conquered another country. What they do is they go fight the war and then when

150
00:13:24,720 --> 00:13:29,320
they pull out the people inside bubble up and change how things operate. Now

151
00:13:29,320 --> 00:13:33,880
that's kind of the MO of stuff. That's the MO of pretty much any nation even

152
00:13:33,880 --> 00:13:41,440
throughout history. So it does make sense that government bodies people who want

153
00:13:41,440 --> 00:13:47,240
to stay employed and one of the ways that people stay employed in the

154
00:13:47,240 --> 00:13:51,240
government is by monitoring the people and knowing what's going on at all times

155
00:13:51,240 --> 00:13:56,440
so that they can better prepare for disasters and you know that sort of

156
00:13:56,440 --> 00:14:02,720
stuff. So I never buy it. I am I'm kind of in the camp of leave me alone I'll

157
00:14:02,720 --> 00:14:11,360
leave you alone and we'll all be happy. Let us just let us all just talk on the

158
00:14:11,360 --> 00:14:15,440
internet play some video games hang out. There's criminals there's always

159
00:14:15,440 --> 00:14:18,560
criminals but you know what there's more good people than there are criminals.

160
00:14:18,560 --> 00:14:27,480
It's just just look at the ratio of people even in you know places that

161
00:14:27,480 --> 00:14:30,720
have governments that aren't the greatest. There's still a huge ratio of

162
00:14:30,720 --> 00:14:38,760
good people. So that's kind of the perspective I have on governments

163
00:14:38,760 --> 00:14:44,880
trying to invest in AI but but there are there are serious consequences to to

164
00:14:44,880 --> 00:14:51,560
this concept of AI trained government systems whether it's in law enforcement

165
00:14:51,560 --> 00:14:59,040
whether it's in you know tracking down people for tax evasion or whether it's

166
00:14:59,040 --> 00:15:03,320
for I don't know just I don't want to walk into a building worrying about my

167
00:15:03,320 --> 00:15:07,680
face being scanned and sent off to to somebody and then me accidentally. I mean

168
00:15:07,680 --> 00:15:12,880
how many how many people have been accidentally jailed even for a short

169
00:15:12,880 --> 00:15:17,760
period of time because messed up you know somebody mistake the identity.

170
00:15:17,760 --> 00:15:21,920
Imagine how much more that would be you know AI is like oh well it's it looks

171
00:15:21,920 --> 00:15:24,800
like the guy because there's I mean there's a lot of people that look like

172
00:15:24,800 --> 00:15:27,520
me and there's a lot of people that look like you. That's just how it is you know

173
00:15:27,520 --> 00:15:33,840
we're all related in some way right. Like I have two parents my wife has two

174
00:15:33,840 --> 00:15:38,880
parents that's already four so you can see where this is going and if you take

175
00:15:38,880 --> 00:15:42,640
if you have two parents for each one of those people right you now have eight

176
00:15:42,640 --> 00:15:48,600
and it just it scales up really fast in just a few generations you would have

177
00:15:48,600 --> 00:15:55,400
more people than all of human history has ever existed in just a few

178
00:15:55,400 --> 00:16:00,520
generations. So yeah a lot of people look like each other it's just how it works.

179
00:16:00,520 --> 00:16:06,480
You can you can maybe tell the difference or in a small group like in

180
00:16:06,480 --> 00:16:12,200
a in a town or something you can really focus on someone and say ah this guy is

181
00:16:12,200 --> 00:16:16,560
highly likely the guy because it looks like him on the camera. You deploy that

182
00:16:16,560 --> 00:16:22,200
like to an FBI database and I walk into a government facility where I live but

183
00:16:22,200 --> 00:16:27,400
the crime was you know ten states over or something and it dings the FBI says

184
00:16:27,400 --> 00:16:31,920
here's your guy because there's 300 million people here there's gonna be

185
00:16:31,920 --> 00:16:38,640
some mess-ups. So I really don't want to see the government getting more into AI

186
00:16:38,640 --> 00:16:43,480
for policing or anything any other reason. I don't think there's any

187
00:16:43,480 --> 00:16:49,080
business for government to be an AI at all. I mean they can that that's why we

188
00:16:49,080 --> 00:16:53,880
have the private sector right so they can pay somebody if your worry is that

189
00:16:53,880 --> 00:16:58,280
hey people are gonna be using AI to do bad things and you need to use AI to

190
00:16:58,280 --> 00:17:05,200
combat it then you have the private sector. We pay taxes you waste our money

191
00:17:05,200 --> 00:17:08,800
on the private sector and they create AI to combat it that's how it goes right

192
00:17:08,800 --> 00:17:14,120
the government doesn't create their war jets some big companies Lockheed Martin

193
00:17:14,120 --> 00:17:19,240
and all those make it so that's just how it works so

194
00:17:19,240 --> 00:17:23,760
Unfortunately, this is a reality of our lifetime,

195
00:17:23,760 --> 00:17:27,080
and it's another reason for us to have private,

196
00:17:27,080 --> 00:17:29,920
local AI, our own personal AI

197
00:17:29,920 --> 00:17:33,680
that we can use to combat all of this nonsense.

198
00:17:33,680 --> 00:17:38,760
We can use it to obfuscate stuff on the Internet,

199
00:17:38,760 --> 00:17:41,600
make us more private, these sorts of things.

200
00:17:41,600 --> 00:17:43,520
Because it's not just about,

201
00:17:43,520 --> 00:17:45,520
I have nothing to hide.

202
00:17:45,520 --> 00:17:48,640
The government hides stuff all the time.

203
00:17:48,640 --> 00:17:50,920
They have their secrets,

204
00:17:50,920 --> 00:17:56,600
and I have my personal life that I don't really want people to know,

205
00:17:56,600 --> 00:17:58,160
because then they can steal my identity,

206
00:17:58,160 --> 00:18:01,840
and they can verify a bunch of parts of my life.

207
00:18:01,840 --> 00:18:07,200
If you're collecting stuff in the government or any other corporation,

208
00:18:07,200 --> 00:18:10,280
mother's maiden name, father's middle name,

209
00:18:10,280 --> 00:18:12,960
favorite pet, you collect all that information.

210
00:18:12,960 --> 00:18:14,960
If somebody bad gets a hold of it,

211
00:18:14,960 --> 00:18:17,000
and they decide to impersonate me,

212
00:18:17,000 --> 00:18:19,560
and my bank says, what's your mother's maiden name?

213
00:18:19,560 --> 00:18:22,160
And they're just like, here it is.

214
00:18:22,160 --> 00:18:24,640
They can really ruin people's lives with that information,

215
00:18:24,640 --> 00:18:28,040
and that's why it's important to keep as much as possible

216
00:18:28,040 --> 00:18:29,880
about our lives private.

217
00:18:29,880 --> 00:18:32,240
That's the argument.

218
00:18:32,240 --> 00:18:34,800
You can't just say you have nothing to hide.

219
00:18:34,800 --> 00:18:38,080
That's the dumbest argument I've ever heard on the planet.

220
00:18:38,080 --> 00:18:39,720
If people argue that,

221
00:18:39,720 --> 00:18:44,320
you're now armed with a counter-argument.

222
00:18:44,320 --> 00:18:47,320
And that's a very valid counter-argument.

223
00:18:47,320 --> 00:18:49,760
Think about how much scamming is happening right now.

224
00:18:49,760 --> 00:18:52,800
So it's a very valid counter-argument.

225
00:18:52,800 --> 00:18:54,880
You don't have anything to hide, yes,

226
00:18:54,880 --> 00:18:57,040
but you should hide things,

227
00:18:57,040 --> 00:19:01,320
because there are people out there that want to abuse your public information,

228
00:19:01,320 --> 00:19:03,840
not just for ads, but to take your money,

229
00:19:03,840 --> 00:19:06,760
maybe take your homes, and put you, you know...

230
00:19:06,760 --> 00:19:09,960
I have known people that have had fraud happen to them,

231
00:19:09,960 --> 00:19:12,440
and it is devastating to their lives.

232
00:19:12,440 --> 00:19:14,960
It is very difficult to get out of it.

233
00:19:14,960 --> 00:19:17,600
They go and they open up bank accounts,

234
00:19:17,600 --> 00:19:19,600
they take all of your money from bank accounts,

235
00:19:19,600 --> 00:19:22,680
they then go to casinos and open up lines of credit,

236
00:19:22,680 --> 00:19:25,880
and it just completely destroys your credit.

237
00:19:25,880 --> 00:19:27,280
How are you going to buy a house with bad credit

238
00:19:27,280 --> 00:19:29,040
because somebody stole your identity?

239
00:19:29,040 --> 00:19:30,040
It's not unheard of.

240
00:19:30,040 --> 00:19:32,360
And the fact that I'm just one random person

241
00:19:32,360 --> 00:19:34,720
in the 300 million in the United States,

242
00:19:34,720 --> 00:19:39,960
and I have a close personal friend who has had to deal with this,

243
00:19:39,960 --> 00:19:42,440
and, you know, what are the odds?

244
00:19:42,440 --> 00:19:46,240
Pretty high, there's people out there now.

245
00:19:46,240 --> 00:19:52,400
Now, you know, it's becoming easier with how much data is being collected.

246
00:19:52,400 --> 00:19:55,520
A lot of that information could just be stolen from a big company

247
00:19:55,520 --> 00:19:58,520
that collected that information without telling you in the first place.

248
00:19:58,520 --> 00:20:02,840
So, government, surveillance, and AI, no-no.

249
00:20:02,840 --> 00:20:05,680
Don't even... if you hear about it, just look into it.

250
00:20:05,680 --> 00:20:07,360
Let's all rally together.

251
00:20:07,360 --> 00:20:09,520
It doesn't matter where you are on the political spectrum.

252
00:20:09,520 --> 00:20:10,960
I really don't care.

253
00:20:10,960 --> 00:20:12,560
But we all can get behind this, right?

254
00:20:12,560 --> 00:20:15,760
No government surveillance with AI, please.

255
00:20:15,760 --> 00:20:18,520
Sounds like a fantastic idea.

256
00:20:18,520 --> 00:20:21,640
And I'll leave it at that.

257
00:20:21,640 --> 00:20:24,920
Okay, so besides all of that,

258
00:20:24,920 --> 00:20:29,640
some of the tech stories that came out,

259
00:20:29,640 --> 00:20:32,720
there's SanDisk Extreme SSDs.

260
00:20:32,720 --> 00:20:34,960
I don't know if you heard about these.

261
00:20:34,960 --> 00:20:39,480
The 4-terabyte models, and I think the 16-terabyte models

262
00:20:39,480 --> 00:20:43,480
they're usually touted as some of the best, you know,

263
00:20:43,480 --> 00:20:45,360
because they're so small and thin.

264
00:20:45,360 --> 00:20:48,120
These tiny little drives that you can take with you

265
00:20:48,120 --> 00:20:50,120
have just been wiping people's data.

266
00:20:50,120 --> 00:20:52,440
It's just... just destroying it.

267
00:20:52,440 --> 00:20:54,720
Just willy-nilly wipe the whole drive.

268
00:20:54,720 --> 00:20:56,520
If you want to put it back on, sure, go ahead.

269
00:20:56,520 --> 00:20:58,800
And then it slowly wipes it.

270
00:20:58,800 --> 00:20:59,800
That's crazy.

271
00:20:59,800 --> 00:21:02,200
What's even crazier is it's not even faulty hardware.

272
00:21:02,200 --> 00:21:03,960
It's a faulty firmware.

273
00:21:03,960 --> 00:21:08,880
So, a firmware, I think they're updating the firmware as we speak.

274
00:21:08,880 --> 00:21:11,800
There's an article here,

275
00:21:11,800 --> 00:21:16,000
SanDisk to release firmware fix for failing Extreme

276
00:21:16,000 --> 00:21:18,200
and Extreme Pro V2 SSDs.

277
00:21:18,200 --> 00:21:21,960
And both of these articles are from today.

278
00:21:21,960 --> 00:21:24,000
So, one is like, it's broken.

279
00:21:24,000 --> 00:21:25,360
The other one's like, they're fixing it.

280
00:21:25,360 --> 00:21:28,440
So, yeah, I think it is a bit extreme

281
00:21:28,440 --> 00:21:34,640
if your little hard drive is wiping everything people put on it.

282
00:21:34,640 --> 00:21:36,920
The whole point of an external drive is

283
00:21:36,920 --> 00:21:39,760
for you to be able to put some stuff on there,

284
00:21:39,760 --> 00:21:41,160
transfer it somewhere else,

285
00:21:41,160 --> 00:21:42,920
keep it on there for a little while.

286
00:21:42,920 --> 00:21:44,640
I don't know.

287
00:21:44,640 --> 00:21:46,320
I would imagine if I put something on a drive,

288
00:21:46,320 --> 00:21:48,720
I'd want it to be there the next month.

289
00:21:48,720 --> 00:21:50,480
It's not like a thumbstick or something.

290
00:21:50,480 --> 00:21:53,960
So, yeah, be aware of that.

291
00:21:53,960 --> 00:21:59,720
If you have those, update your firmware

292
00:21:59,720 --> 00:22:04,480
and make sure that your data is still on your drives.

293
00:22:04,480 --> 00:22:14,160
Now, I think that some more stuff related to AI and tech

294
00:22:14,160 --> 00:22:17,800
is more of a moralistic problem.

295
00:22:17,800 --> 00:22:19,520
It's a training problem.

296
00:22:19,520 --> 00:22:23,960
So, we have, there's been this problem with YouTube,

297
00:22:23,960 --> 00:22:27,840
with Vimeo, with any web video,

298
00:22:27,840 --> 00:22:32,120
and also with text and stories and all this sort of stuff

299
00:22:32,120 --> 00:22:35,200
where somebody, there's a human somewhere

300
00:22:35,200 --> 00:22:39,200
that has to look at this stuff.

301
00:22:39,200 --> 00:22:41,960
And it could be mutilation and murder

302
00:22:41,960 --> 00:22:45,480
and all kinds of really bad things going on in these videos

303
00:22:45,480 --> 00:22:48,160
and these descriptions and text and stuff.

304
00:22:48,160 --> 00:22:51,680
It goes to the web that need to be filtered by a person.

305
00:22:51,680 --> 00:22:54,480
Now, of course, we have a lot of AI that goes in

306
00:22:54,480 --> 00:22:57,520
and detects a lot of that stuff,

307
00:22:57,520 --> 00:22:59,960
but somebody had to train it, the AI,

308
00:22:59,960 --> 00:23:02,680
by looking explicitly at all this stuff.

309
00:23:02,680 --> 00:23:05,520
And I think that a lot of companies

310
00:23:05,520 --> 00:23:08,640
outsource this to other countries

311
00:23:08,640 --> 00:23:11,920
just because it's cheaper.

312
00:23:11,920 --> 00:23:15,160
And frankly, that's basically the only reason, I assume.

313
00:23:15,160 --> 00:23:19,440
And these people are, they get traumatized.

314
00:23:19,440 --> 00:23:23,320
A lot of people, you know, there are a lot of people now

315
00:23:23,320 --> 00:23:25,480
with what kind of websites exist

316
00:23:25,480 --> 00:23:27,920
that traumatize themselves for enjoyment.

317
00:23:27,920 --> 00:23:30,080
But a lot of these people who just wanna make some money

318
00:23:30,080 --> 00:23:33,920
for their family, who wanna work hard and, you know,

319
00:23:33,920 --> 00:23:37,240
just provide, they have to do these kinds of jobs

320
00:23:37,240 --> 00:23:38,080
and it's traumatizing.

321
00:23:38,080 --> 00:23:39,520
They don't wanna be exposed to all this stuff.

322
00:23:39,520 --> 00:23:41,920
And it's a hard question.

323
00:23:41,920 --> 00:23:44,520
I brought this one up because I am training some AI stuff

324
00:23:44,520 --> 00:23:45,720
and it's a hard question.

325
00:23:45,720 --> 00:23:48,160
How do you avoid having to look at all this stuff,

326
00:23:48,160 --> 00:23:51,240
but also have to look at it in order to train things?

327
00:23:51,240 --> 00:23:55,440
Now, it's my hope that one day we'll have an AI model

328
00:23:55,440 --> 00:23:58,520
specifically trained on all of this nastiness

329
00:23:58,520 --> 00:24:00,400
so that it can deal with it.

330
00:24:00,400 --> 00:24:03,120
And that would be fantastic.

331
00:24:03,120 --> 00:24:06,720
And I think that we do have some that people are working on.

332
00:24:06,720 --> 00:24:10,800
Ironically, there's a bunch of NSFW kind of models

333
00:24:10,800 --> 00:24:13,280
people are training and putting out there.

334
00:24:13,280 --> 00:24:15,680
And those could be used potentially

335
00:24:15,680 --> 00:24:17,880
for detecting this kind of stuff.

336
00:24:17,880 --> 00:24:22,480
So I do wonder if there's a way that we can,

337
00:24:22,480 --> 00:24:26,280
like, you know that story where the person got a paperclip

338
00:24:26,280 --> 00:24:27,560
and they wound up with a house in the end

339
00:24:27,560 --> 00:24:29,320
because they traded the paperclip for a pencil

340
00:24:29,320 --> 00:24:32,640
and then traded the pencil for, I don't know,

341
00:24:32,640 --> 00:24:33,800
some other thing.

342
00:24:33,800 --> 00:24:34,680
It was never money.

343
00:24:34,680 --> 00:24:37,680
And then they wound up with a car, a house, something big,

344
00:24:37,680 --> 00:24:39,040
all from a paperclip.

345
00:24:39,040 --> 00:24:42,600
I feel like maybe we can make that paperclip AI

346
00:24:42,600 --> 00:24:46,280
and then kind of train it up in some way

347
00:24:46,280 --> 00:24:48,160
that it trains itself and it trains the next one

348
00:24:48,160 --> 00:24:49,000
and the next one trains it.

349
00:24:49,000 --> 00:24:50,280
And we kind of go back and forth

350
00:24:50,280 --> 00:24:52,600
with this adversarial kind of training.

351
00:24:53,720 --> 00:24:57,320
That way, we don't have to expose ourselves to all this.

352
00:24:57,320 --> 00:24:58,840
It's an interesting moral question.

353
00:24:58,840 --> 00:25:01,560
I am curious to see what people think about how to solve it

354
00:25:01,560 --> 00:25:03,200
because I don't think we should expose people

355
00:25:03,200 --> 00:25:04,240
to all this stuff.

356
00:25:04,240 --> 00:25:07,320
But at the same time, some people,

357
00:25:07,320 --> 00:25:09,040
there will have to be a human element to it

358
00:25:09,040 --> 00:25:10,760
and somebody has to make the sacrifice.

359
00:25:10,760 --> 00:25:13,640
And of course, there's always been people

360
00:25:13,640 --> 00:25:16,280
throughout all of history that have made

361
00:25:16,280 --> 00:25:18,200
a mental and physical sacrifice

362
00:25:18,200 --> 00:25:20,720
for the betterment of the people that come after

363
00:25:20,720 --> 00:25:21,840
for their children.

364
00:25:21,840 --> 00:25:26,400
And so it could be the cost and somebody's got to pay it.

365
00:25:26,400 --> 00:25:28,120
It's an interesting problem to solve.

366
00:25:28,120 --> 00:25:31,480
I think if somebody can accelerate solving that

367
00:25:31,480 --> 00:25:34,240
so that we can get people off of manually training

368
00:25:34,240 --> 00:25:37,920
all these models on this really bad stuff

369
00:25:37,920 --> 00:25:41,120
that just shouldn't exist, the better.

370
00:25:43,200 --> 00:25:47,960
So that's a lot of questions I have on that.

371
00:25:47,960 --> 00:25:52,960
But now we're going to move on to security stuff.

372
00:25:54,160 --> 00:25:58,240
I have some security updates for people,

373
00:25:58,240 --> 00:26:02,400
but I also have some security warnings for people.

374
00:26:02,400 --> 00:26:04,960
We already talked about people's secure data

375
00:26:04,960 --> 00:26:09,440
and transferring it to big corporations.

376
00:26:09,440 --> 00:26:11,480
So why not add in some more?

377
00:26:11,480 --> 00:26:14,720
Amazon has now some palm scanning technology

378
00:26:14,720 --> 00:26:19,400
that lets you buy a drink or groceries or anything else.

379
00:26:19,400 --> 00:26:23,000
You scan your palm and it purchases stuff

380
00:26:23,000 --> 00:26:27,120
probably using Amazon Pay or verifies your identity

381
00:26:27,120 --> 00:26:31,040
and it also verifies your age and all that sort of stuff

382
00:26:31,040 --> 00:26:34,160
if you want to buy things that are age restricted.

383
00:26:34,160 --> 00:26:37,120
And so

384
00:26:37,120 --> 00:26:43,360
Amazon itself, it's kind of not surprising that they move into that.

385
00:26:43,360 --> 00:26:47,820
They're a purchasing company where you buy commodities,

386
00:26:47,820 --> 00:26:52,200
and they're also a tech company with AWS and all their server stuff.

387
00:26:52,200 --> 00:27:01,160
So it makes sense that they would continue to try and combine tech and payments.

388
00:27:01,160 --> 00:27:04,880
So obviously, I don't even have to say it,

389
00:27:04,880 --> 00:27:10,480
though the big problem with this is you're associating your ID

390
00:27:10,480 --> 00:27:15,560
and your now physical body even more with corporations.

391
00:27:15,560 --> 00:27:19,400
And a lot of biometric tech is local, right?

392
00:27:19,400 --> 00:27:25,200
So you have IR cameras on your laptop for Windows Hello.

393
00:27:25,200 --> 00:27:27,920
Who knows, they probably store it in some server somewhere.

394
00:27:27,920 --> 00:27:32,680
But you can unlock your computer without an internet connection using it.

395
00:27:32,680 --> 00:27:37,640
So these biometric things exist,

396
00:27:37,640 --> 00:27:44,480
but now we're starting to talk about using them for purchases with corporations.

397
00:27:44,480 --> 00:27:46,400
And that gets kind of weird.

398
00:27:46,400 --> 00:27:48,880
I feel like they already have enough data,

399
00:27:48,880 --> 00:27:55,320
but now they want your palm friends, your iris scans, your facial recognition,

400
00:27:55,320 --> 00:27:59,280
your ID cards, they want your toe print at some point.

401
00:27:59,280 --> 00:28:02,480
And so it's getting a little much.

402
00:28:02,480 --> 00:28:06,480
And I'd like to see some pushback on this sort of stuff.

403
00:28:06,480 --> 00:28:11,440
I want people to say, no, I'm not going to scan my palm.

404
00:28:11,440 --> 00:28:14,480
I mean, what's the difference between scanning your palm

405
00:28:14,480 --> 00:28:17,440
and just taking out your phone and scanning your phone

406
00:28:17,440 --> 00:28:23,200
whenever you use your card NFC reader?

407
00:28:23,200 --> 00:28:25,920
So really, I don't see the advantage here.

408
00:28:25,920 --> 00:28:28,480
It's not like it makes things that much faster.

409
00:28:28,480 --> 00:28:30,760
Most people already have their phones in their hands,

410
00:28:30,760 --> 00:28:34,120
so why not just bloop?

411
00:28:34,120 --> 00:28:37,400
I mean, is it really that hard?

412
00:28:37,400 --> 00:28:41,920
Is it really that hard to pull your wallet out and scan a card

413
00:28:41,920 --> 00:28:45,400
for a huge advantage in privacy?

414
00:28:45,400 --> 00:28:47,240
It seems kind of weird to me.

415
00:28:47,240 --> 00:28:51,080
And I hope this isn't the way that everyone kind of pushes for,

416
00:28:51,080 --> 00:28:55,280
is more of this biometric identification.

417
00:28:55,280 --> 00:28:58,160
I don't want to one day walk down the street

418
00:28:58,160 --> 00:29:02,760
and then some kind of billboard scans my face

419
00:29:02,760 --> 00:29:07,120
and realizes who I am and then swaps to an advertisement for me.

420
00:29:07,120 --> 00:29:10,920
And then they'll be like, oh, it's super private.

421
00:29:10,920 --> 00:29:15,840
Only at your viewing angle can you see these pixels that are designed.

422
00:29:15,840 --> 00:29:18,400
Everyone sees their own unique billboard.

423
00:29:18,400 --> 00:29:19,680
I don't know.

424
00:29:19,680 --> 00:29:22,080
You can guarantee that's going to be some kind of nonsense

425
00:29:22,080 --> 00:29:23,120
that's going to come up.

426
00:29:23,120 --> 00:29:28,200
Yeah, I don't want to integrate my presence

427
00:29:28,200 --> 00:29:32,240
into corporations and all that sort of stuff.

428
00:29:32,240 --> 00:29:37,320
I'm okay with all of this disappearing once I get put six feet under.

429
00:29:37,320 --> 00:29:40,560
So it's fine with me.

430
00:29:40,560 --> 00:29:43,520
But yeah, a little bit of security there.

431
00:29:43,520 --> 00:29:47,640
If you've seen these things at wherever Amazon is placing these,

432
00:29:47,640 --> 00:29:52,360
just say no and pull out your ID or your card or whatever.

433
00:29:52,360 --> 00:29:57,560
It's a good idea just to say no sometimes.

434
00:29:57,560 --> 00:30:02,160
A little PSA for people who have Samsung phones,

435
00:30:02,160 --> 00:30:05,400
like I do, I have three different variants

436
00:30:05,400 --> 00:30:07,200
just right here on this desk.

437
00:30:07,200 --> 00:30:08,600
I'm not a fanboy or anything.

438
00:30:08,600 --> 00:30:14,640
I just like Android phones and Samsung has nice phones.

439
00:30:14,640 --> 00:30:20,600
So anyways, it affects Samsung devices running

440
00:30:20,600 --> 00:30:24,280
for Android versions 11, 12, and 13.

441
00:30:24,280 --> 00:30:26,400
There's a...

442
00:30:26,400 --> 00:30:29,200
Okay, so when you load up an app on your phone

443
00:30:29,200 --> 00:30:34,280
and on some computers, or I guess most computers,

444
00:30:34,280 --> 00:30:40,120
it'll take that application and it will scramble it

445
00:30:40,120 --> 00:30:42,360
in a way that you can't figure out where it's at.

446
00:30:42,360 --> 00:30:44,960
So traditionally on old school computers,

447
00:30:44,960 --> 00:30:49,160
your program would load and it would be put into a part of your RAM

448
00:30:49,160 --> 00:30:53,320
and it would read instruction by instruction.

449
00:30:53,320 --> 00:30:56,240
And anything that knows where that program is

450
00:30:56,240 --> 00:30:58,680
can actually alter it and mess with it.

451
00:30:58,680 --> 00:31:02,840
So they had to create ways of preventing access

452
00:31:02,840 --> 00:31:06,760
from other programs, whether it's permissions or security,

453
00:31:06,760 --> 00:31:10,440
but also through obfuscation where it loads up your program

454
00:31:10,440 --> 00:31:12,240
and it scrambles where it's at.

455
00:31:12,240 --> 00:31:14,840
Maybe it separates it out into different parts of memory

456
00:31:14,840 --> 00:31:20,760
or it has an addressing, this is where the app is,

457
00:31:20,760 --> 00:31:22,800
but it goes through a layer that reroutes it,

458
00:31:22,800 --> 00:31:24,200
that reroutes to something else.

459
00:31:24,200 --> 00:31:26,600
And so it's hard to figure out where the app is.

460
00:31:26,600 --> 00:31:30,440
So there's a bug on the Samsung devices on 11, 12, and 13

461
00:31:30,440 --> 00:31:34,400
that doesn't do that rerouting for the application.

462
00:31:34,400 --> 00:31:38,520
So this is not a big security risk for a lot of people.

463
00:31:38,520 --> 00:31:40,880
It's just keep your phone updated.

464
00:31:40,880 --> 00:31:42,040
Let me see when this was.

465
00:31:42,040 --> 00:31:44,600
May 19th, so keep your phone updated.

466
00:31:44,600 --> 00:31:50,440
I think they'll be putting out a update in June, I think it said.

467
00:31:50,440 --> 00:31:53,720
June 9th is when there are expected to be patches

468
00:31:53,720 --> 00:31:55,640
or they'd hope for patches.

469
00:31:55,640 --> 00:31:58,760
So just look out for an update, keep your phone up to date.

470
00:31:58,760 --> 00:32:01,800
Not super critical of a bug.

471
00:32:01,800 --> 00:32:04,080
You have to already be infected with something for it

472
00:32:04,080 --> 00:32:04,920
to abuse this.

473
00:32:04,920 --> 00:32:10,120
So not likely a target vector many people would,

474
00:32:10,120 --> 00:32:14,640
many hackers would aim for.

475
00:32:14,640 --> 00:32:17,160
Another thing in security that happened recently

476
00:32:17,160 --> 00:32:19,240
is PyPi repositories.

477
00:32:19,240 --> 00:32:20,700
And if you don't know what this is,

478
00:32:20,700 --> 00:32:22,720
people use Python programming language.

479
00:32:22,720 --> 00:32:25,560
And if you use any language, like C Sharp and NuGet

480
00:32:25,560 --> 00:32:31,240
or whatever, any kind of package manager for programmers,

481
00:32:31,240 --> 00:32:33,320
we have this problem where programmers are lazy.

482
00:32:33,320 --> 00:32:35,160
And so they just download a bunch of packages

483
00:32:35,160 --> 00:32:36,240
to do stuff for them.

484
00:32:36,240 --> 00:32:38,080
They'll read the description and be like, ah, that's great.

485
00:32:38,080 --> 00:32:39,920
Download it and start using it.

486
00:32:39,920 --> 00:32:42,960
And these packages do take time for people

487
00:32:42,960 --> 00:32:45,680
to audit their security.

488
00:32:45,680 --> 00:32:48,960
And they use algorithms to find common ones.

489
00:32:48,960 --> 00:32:51,600
But if somebody just programmed up one and put it up

490
00:32:51,600 --> 00:32:57,040
as a package, it could take some time before they are found.

491
00:32:57,040 --> 00:33:03,280
And so this repository for PyPi, our many repositories

492
00:33:03,280 --> 00:33:06,400
that were kind of linked to by PyPi,

493
00:33:06,400 --> 00:33:09,560
started having all kinds of malicious hardware.

494
00:33:09,560 --> 00:33:12,840
They are malicious software, malicious packages

495
00:33:12,840 --> 00:33:15,520
or whatever you want to call them, were being uploaded.

496
00:33:15,520 --> 00:33:17,200
And people were downloading these.

497
00:33:17,200 --> 00:33:22,080
One of them was downloaded 1,200 times before it got caught.

498
00:33:22,080 --> 00:33:25,200
And PyPi actually had to shut down signups and everything

499
00:33:25,200 --> 00:33:27,840
else over the weekend for 17 hours

500
00:33:27,840 --> 00:33:29,560
just to clean all this crap up.

501
00:33:29,560 --> 00:33:33,160
So don't download packages unless you know it's good

502
00:33:33,160 --> 00:33:35,160
or you audited it yourself or it's

503
00:33:35,160 --> 00:33:38,240
been audited by a security professional or someone

504
00:33:38,240 --> 00:33:39,120
you trust.

505
00:33:39,120 --> 00:33:42,680
So it's not that hard to write some code.

506
00:33:42,680 --> 00:33:44,400
Just don't just download anything.

507
00:33:44,400 --> 00:33:46,760
There are a couple of examples where

508
00:33:46,760 --> 00:33:50,560
there was a package named Node.js Encrypt Agent and Node.js

509
00:33:50,560 --> 00:33:55,360
Cookie Proxy Agent, which is a funny Node.js thing in Python.

510
00:33:55,360 --> 00:33:57,520
And that was the one that was downloaded 1,200 times

511
00:33:57,520 --> 00:33:59,680
before it got caught.

512
00:33:59,680 --> 00:34:01,720
And then in March this year, there

513
00:34:01,720 --> 00:34:06,960
was another malicious package called Colorful with a U.

514
00:34:06,960 --> 00:34:10,560
Colorful on PyPi.

515
00:34:10,560 --> 00:34:14,760
It was discovered being distributed.

516
00:34:14,760 --> 00:34:17,800
It was malware.

517
00:34:17,800 --> 00:34:20,120
It was referred to as colorblind.

518
00:34:20,120 --> 00:34:23,800
I guess they were using, what's it called,

519
00:34:23,800 --> 00:34:27,480
some accessibility verbiage.

520
00:34:27,480 --> 00:34:30,840
And people were just downloading it.

521
00:34:30,840 --> 00:34:34,240
So yeah, if you are a programmer and you're out there programming

522
00:34:34,240 --> 00:34:37,800
away, don't just download packages.

523
00:34:37,800 --> 00:34:40,080
Use vetted ones or write it yourself.

524
00:34:40,080 --> 00:34:41,920
It's not that hard to write your own code.

525
00:34:41,920 --> 00:34:43,560
A lot of these, we're targeting people

526
00:34:43,560 --> 00:34:45,840
to try and copy from their clipboard

527
00:34:45,840 --> 00:34:49,960
in order to get cryptocurrency wallets and that sort of stuff.

528
00:34:49,960 --> 00:34:52,680
Cryptocurrency is a big vector for people stealing stuff

529
00:34:52,680 --> 00:34:54,920
because, poof, it's gone.

530
00:34:54,920 --> 00:34:55,920
Who's going to trace it?

531
00:34:55,920 --> 00:34:57,120
Nobody's going to know.

532
00:34:57,120 --> 00:35:00,200
So yeah, that's just an update for programmers

533
00:35:00,200 --> 00:35:03,760
to keep an eye out for.

534
00:35:03,760 --> 00:35:11,280
And on the coding front, I just discovered

535
00:35:11,280 --> 00:35:14,080
people use online coding.

536
00:35:14,080 --> 00:35:17,160
Am I so old that I'm the offline coding guy?

537
00:35:17,160 --> 00:35:22,000
I'm the guy that uses dinosaur code or something?

538
00:35:22,000 --> 00:35:25,120
There's articles about online coding

539
00:35:25,120 --> 00:35:27,080
and the advantages of it.

540
00:35:27,080 --> 00:35:28,920
And then there's counter articles

541
00:35:28,920 --> 00:35:31,480
about why you should use offline coding.

542
00:35:31,480 --> 00:35:33,880
And this was the first I've ever heard of this.

543
00:35:33,880 --> 00:35:35,800
I get that you can use the Visual Studio

544
00:35:35,800 --> 00:35:39,240
code, basically, inside of GitHub or whatever.

545
00:35:39,240 --> 00:35:40,520
You could pay for their thing.

546
00:35:40,520 --> 00:35:41,640
I don't know why you would.

547
00:35:41,640 --> 00:35:43,920
I mean, you've got to use a computer to access it.

548
00:35:43,920 --> 00:35:45,240
Your computer can't be that bad.

549
00:35:45,240 --> 00:35:47,480
No, it can't run Visual Studio code.

550
00:35:50,360 --> 00:35:52,200
But apparently, this is the thing.

551
00:35:52,200 --> 00:35:55,480
Online coding.

552
00:35:55,480 --> 00:35:58,720
If you are an online coder, tell me what that means.

553
00:35:58,720 --> 00:36:03,280
I read these articles, and I don't fully understand it.

554
00:36:03,280 --> 00:36:07,280
Some of the arguments were offline coding is more secure.

555
00:36:07,280 --> 00:36:10,160
You don't have to worry about internet distractions.

556
00:36:10,160 --> 00:36:16,560
It seems like it seems kind of, you know, I don't know.

557
00:36:16,560 --> 00:36:19,480
Maybe new coders, coders who are learning to program

558
00:36:19,480 --> 00:36:21,240
or coming from another field, people

559
00:36:21,240 --> 00:36:24,720
who haven't drudged through the mud

560
00:36:24,720 --> 00:36:30,280
and the sludge of programming to really say

561
00:36:30,280 --> 00:36:35,200
that they have to use offline systems for development.

562
00:36:35,200 --> 00:36:37,440
Maybe it's those people.

563
00:36:37,440 --> 00:36:38,920
Maybe it's web developers.

564
00:36:38,920 --> 00:36:40,880
It's just a lot easier to do stuff on the web.

565
00:36:40,880 --> 00:36:42,760
I don't know.

566
00:36:42,760 --> 00:36:44,760
But yeah, use offline coding.

567
00:36:44,760 --> 00:36:46,960
In fact, turn off your internet when you code.

568
00:36:46,960 --> 00:36:48,000
Just code.

569
00:36:48,000 --> 00:36:50,760
Have some, just breathe.

570
00:36:50,760 --> 00:36:53,600
Drink some tea or whatever it is you drink.

571
00:36:53,600 --> 00:36:58,240
Soda, Coke Zero, and relax and do some code.

572
00:36:58,240 --> 00:37:00,320
It's therapeutic.

573
00:37:00,320 --> 00:37:02,760
You don't need the internet to code.

574
00:37:02,760 --> 00:37:04,320
I know that you do like Stack Overflow.

575
00:37:04,320 --> 00:37:08,720
And of course, now you like your GitHub Copilot.

576
00:37:08,720 --> 00:37:10,320
I mean, I like GitHub Copilot.

577
00:37:10,320 --> 00:37:15,320
I use it for just fooling around and learning languages

578
00:37:15,320 --> 00:37:15,800
and stuff.

579
00:37:15,800 --> 00:37:18,560
So I mean, I get that appeal.

580
00:37:18,560 --> 00:37:21,920
But I've spent most of my time coding essentially

581
00:37:21,920 --> 00:37:23,240
without an internet connection.

582
00:37:23,240 --> 00:37:25,400
So it's very nice.

583
00:37:25,400 --> 00:37:31,040
So that's about it that I have for today.

584
00:37:31,040 --> 00:37:33,200
Not a whole lot of stuff.

585
00:37:33,200 --> 00:37:35,400
But it was a lot.

586
00:37:35,400 --> 00:37:38,200
I mean, it's kind of the stuff that happened recently.

587
00:37:38,200 --> 00:37:39,920
And there's a lot of security stuff.

588
00:37:39,920 --> 00:37:41,420
Just pay attention to your security

589
00:37:41,420 --> 00:37:42,840
and pay attention to your privacy.

590
00:37:42,840 --> 00:37:47,840
Use apps like Signal to talk to your family and friends.

591
00:37:47,840 --> 00:37:52,080
Just don't even log into Facebook, that sort of stuff.

592
00:37:52,080 --> 00:37:53,280
And have a good one.

593
00:37:53,280 --> 00:38:18,080
So bye for now.

